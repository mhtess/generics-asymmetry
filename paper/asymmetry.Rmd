---
title             : "The asymmetry between generic truth conditions and implied prevalence"
shorttitle        : "Generic asymmetry"

author: 
  - name          : "Michael Henry Tessler"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Serra Mall, Bldg. 420, Rm. 316, Stanford, CA 94305"
    email         : "mhtessler@stanford.edu"
  - name          : "Noah D. Goodman"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"
    
header-includes:
  - \usepackage{tabularx}
  - \usepackage{multicol}
  - \usepackage{wrapfig}
  - \usepackage{caption}
  - \usepackage{booktabs}
  - \usepackage{amsmath}
  - \usepackage{graphicx}
  - \usepackage{subcaption}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  
author_note: >
  This manuscript is currently in prep. Comments or suggestions should be directed to MH Tessler.

abstract: |
  Enter abstract here. Each new line herein must be indented, like this line.
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["generics.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no

class             : "man"
output            : papaja::apa6_pdf
---
\newcommand{\denote}[1]{\mbox{ $[\![ #1 ]\!]$}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\definecolor{Red}{RGB}{255,0,0}
\definecolor{Green}{RGB}{10,200,100}
\definecolor{Blue}{RGB}{10,100,200}

\newcommand{\mht}[1]{{\textcolor{Blue}{[mht: #1]}}}
\newcommand{\ndg}[1]{{\textcolor{Green}{[ndg: #1]}}}
\newcommand{\red}[1]{{\textcolor{Red}{#1}}}


```{r load_packages, include = FALSE}
library("papaja")
library(tidyverse)
library(cowplot)
library(ggthemes)
library(RColorBrewer)
library(ggpirate)
theme_set(theme_few())

format_regression_effects <- function(brm_summary, fixed_effect_name, n_digits = 3){
   #print(fixed_effect_name)
   e1 <- brm_summary[["fixed"]][[fixed_effect_name, "Estimate"]]
   e_lower <- brm_summary[["fixed"]][[fixed_effect_name, "l-95% CI"]]
   e_upper <- brm_summary[["fixed"]][[fixed_effect_name, "u-95% CI"]]
   return(paste(
     format(e1, digits = n_digits), " [", 
     format(e_lower, digits = n_digits), ", ", 
     format(e_upper, digits = n_digits), "]", sep = ""))
}

```

```{r analysis_preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, cache=T, message=FALSE, sanitize = T)

```

# Introduction

<!-- 
- Note about large-scale truth judgment task: 
  - Cimpian fudges the analysis a little bit: He does it on a subject-wise basis, but using different properties (participants dont rate purple feathers for 10%, 30%, ...)... if we want to do item-wise, we either need to give subjects all the prevalence levels, or do some fancy hierarchical bayesian modeling (the latter may be preferred)
-->


<!-- Generic statements (e.g., "Birds fly") convey generalizations about categories [@Carlson1977; @Carlson1995; @Leslie2008]. -->
<!-- This kind of language has caught the attention of psychologists, linguists, and philosophers beacuse it is widely prevalent in everyday conversation and child-directed speech [@Gelman1998; @Gelman2008; @GelmanEtAl2004], while at the same time having formal properties that are difficult to characterize [@Carlson1995].  -->

In Chapter 3, we saw how generic statements can be judged as true under a wide range of statistical conditions. 
"Birds fly" is true of most birds, but "Ducks lay eggs" is only true of adult, fertile, female ducks, which make up less than 50\% of the category.
"Mosquitos carry malaria" is intuitively true despite less than 1\% of category actually having the property.
Despite these variable truth conditions, it is thought that generic statements often carry the *implied prevalence* of a universal or near-universal quantifier [@Abelson1966; @Gelman2002; @Cimpian2010]. 
For example, when both children and adults learn "Bears like to eat ants", they tend to think that *almost all* bears like to eat ants [@Gelman2002].

These two intuitions lead to an asymmetry: Generics have flexible truth conditions but are interpreted strongly. 
@Cimpian2010 found that upon reading a novel generic sentence (e.g., "Glippets have yellow fur."), participants infer that the implied referent prevalence is high (e.g., almost all glippets have yellow fur).
At the same time, when participants are confronted with evaluating the same statement based on weak statistical evidence (e.g., 30% of glippets have yellow fur), some participants are still willing to endorse the statement.

The asymmetry is worrisome from a societal perspective.
Generics are a primary vehicle for conveying stereotypes [@GelmanEtAl2004]. 
The fact that listeners infer higher prevalence than speakers had in mind would explain why such stereotypes get propogated, even when the actual prevalence may be low. 

The asymmetry is puzzling from a theoretical persepective as well.
We know from Chapters 2 & 3 that both endorsements and interpretations display sensitivity to the property in precise quantitative ways. 
But an asymmetry would indicate that the context-sensitivity of endorsements is different from that of interpretations.

What is the relationship between generic truth conditions and implied prevalence? 
Here, we show that the asymmetry that was reported by @Cimpian2010 is biased in systematic ways resulting from averaging one source of data (endorsement judgments) in order to compare to the other (implied prevalence). 
Instead, we propose comparing the measurements by weighting endorsements by *need probabilities* [@Kemp2018], that is, the probability with which a speaker would have to refer to a kind of a particular type.
Weighting by need probabilities better embodies the goal of characterizing the situations under which a speaker would endorse a generic.

<!-- We argue that weighting by need probabilities better summarizes the context-sensitivity of endorsement judgments by displaying much more quantitative variation. -->
<!-- We find that, using this measure, there should be no asymmetry in theory.  -->
<!-- We also find a relatively high correlation between average endorsed prevalence and implied prevalence, suggesting that  -->

We show that the reported asymmetry is driven primarily by differences in implied prevalence, which we already explored in Chapter 2. 
The endorsed prevalence (a composite measure used to compare to implied prevalence) shows substantially less variability across items, and thus, also reduced test-retest reliability. 
Even so, our endorsement model is still able to explain a significant amount of variance in the average endorsed prevalence.
We conclude by discussing other mechanisms of *evidence exaggeration*.



<!-- When and how does the asymmetry between truth conditions and implied prevalence manifest? -->
<!-- <!-- \red{[check cimpian's discussion section]} PERHAPS ADDRESS LATER WHAT CIMPIAN SAYS, because it's not directly the point (his ideas are tooo vague) -->
<!-- Here, we draw on the same computational framework to address this question.  -->
<!-- First, we describe how endorsements and implied prevalence are compared and show through simulation various conditions under which the asymmetry appears and when it is predicted to disappear or even reverse. -->
<!-- Then, we replicate the original asymmetry findings of @Cimpian2010 while expanding the stimulus set to reveal even more variability in the asymmetry. -->
<!-- We then extend these findings to a larger-scale study using a more diverse stimulus set. -->
<!-- In addition to revealing the quantitative intracacies of the asymmetry, our very same model is able to account for both truth conditions and implied prevalence simultaneously.  -->



<!-- What is the source of the asymmetry?  -->
<!-- @Cimpian2010 (Expt. 3) found a significant reduction in the asymmetry for generics about accidental properties (e.g., "Glippets have wet fur."), suggesting that the asymmetry is not a purely semantic phenomenon (i.e., not arising just out of the meaning of generics).  -->

<!-- our knowledge of properties is at least partially responsible for the asymmetry. -->

<!-- But generic endorsements are also sensitive to the properties.  -->


<!-- mostly driven by those statements carrying substantially weaker interpretations.  -->

<!-- Of course, generics need not imply high prevalence. -->
<!-- We saw in Chapter 2 that statements like "Feps live in zoos" are interpreted by adults as applying to relatively few feps, while "Feps eats insects" is taken to apply *almost all* feps.  -->
<!-- Accordingly,  -->
<!-- Endorsements for generics are similarly sensitive to the property (e.g., "Birds lay eggs" vs. "Birds are female"), but the results of @Cimpian2010 suggest the sensitivity of endorsement is *less* than that of interpretations. -->



<!-- Is the source of the asymmetry a semantic one?  -->
<!-- That is, does the uncertain threshold meaning for generics produce an asymmetry between endorsements and interpretations?  -->
<!-- Or, like other phenomena in generic language, is the result of background knowledge about properties? -->

<!-- When we examine our intuition about the asymmetry, it seems to come from different example sentences: "Mosquitos carry malaria" is endorsed at low prevalence vs. "Dogs bark" implies high prevalence.  -->
<!-- The sensitivity of implied prevalence, we showed, was a result of diverse prior knowledge about properties interacting with the uncertain semantics of generics.  -->




<!-- Upon hearing "Feps eat insects", listeners come to think *almost all* feps eat insects; the possibility that only 10% of them eat insects does not immediately come to mind.  -->
<!-- However, if we *knew* that 10% of feps ate insects, the generic "Feps eat insects" does not seem at all false.  -->

<!-- By contrast, participants endorse generics for a wide range of prevalence levels (e.g., even when 30\% of glippets have yellow fur).  -->
<!-- By quantitatively comparing the two measurements, @Cimpian2010 found an asymmetry between between truth conditions and implied prevalence of generic statements but not for quantified statementse involving *all* or *most*. -->

<!-- Not all generics have strong interpretations, however.  -->
<!-- "Mosquitos carry malaria" really seems to only have the quantificational force of an existential claim, meaning *some \mosquitos carry malaria*. -->
<!-- Weak interpretations from generics have been observed in tasks having participants draw inferences about individual members of a kind [@Khemlani2009; @Khemlani2012] as well as asking about implied prevalence [@Cimpian2010; Expt. 3]. -->
<!-- Coupled with the truth conditions measurements, @Cimpian2010 found that a significant reduction in the asymmetry for generics about accidental properties (e.g., "Glippets have wet fur."). -->

# Theoretical analysis

The asymmetry between endorsements and interpretations results from comparing participants' responses on  a two-alternative forced choice (2AFC) truth judgment task (as explored in Chapter 3) to implied prevalence ratings, typically on a 101-pt scale (e.g., the percentage of the category with the property; Chapter 2).
These two measurements are not directly comparable because of the different scales.

In order to compare the two, @Cimpian2010 took the 2AFC endorsement data and computed the average prevalence level at which individual participants endorsed the generic, what I will call *average endorsed prevalence*.
A single participant's average endorsed prevalence was computed by the mean of the prevalence levels for which the participant endorsed the generic. 
For example, if a participant endorsed the generic at 50%, 70%, and 90% prevalence levels, their *average endorsed prevalence* would be 70%. 
As a participant becomes more lenient with the generic (endorsing it at lower prevalence levels), their average endorsed prevalence decreases; as a participant becomes more conservative (e.g., only endorsing the generic when the property applies to *all* or *almost all*), their average endorsed prevalence increases. 
The theoretical minimum average endorsed prevalence is 50%, for a participant who endorsed the generic at all prevalence levels.^[
  Technically, average endorsed prevalence could be less than 50% were a participant to endorse the statement at low prevalence levels and then reject the statement at high prevalence levels. 
]
If a participant fails to endorse the generic for all prevalence levels, @Cimpian2010 assigned them an average endorsed prevalence of 100%, because presumably the participant would only endorse the generic if it were true of 100% of members.
<!-- @Cimpian2010's analysis transforms the endorsement data onto the prevalence scale and behaves intuitively to how one would think the construct should behave. -->
<!-- As a participant becomes more lenient with the generic (endorsing it at lower prevalence levels), their average endorsed prevalence decreases; as a participant becomes more conservative (e.g., only endorsing the generic when the property applies to *all* or *almost all*), their average endorsed prevalence increases.  -->

With the exception of how the analysis handles participants who never endorse the statement, @Cimpian2010's analysis is an application of Bayes' Theorem.
Consider that each task each measures a conditional probability. 
The implied prevalence task measures the probability of different prevalence levels $r$ given a generic statement $u$: $\hat{P}_{implied}(r \mid u)$. 
(We use $\hat{P}$ to denote an empirically measured probability and $P$ to denote a theoretical probability.)
The endorsement tasks measure the probability of endorsing a generic utterance $u$ given different prevalence levels: $\hat{P}_{endorsed}(u \mid r)$. 
These two conditional probabilities are related via Bayes' rule: $P(r \mid u) \propto P(u \mid r) \cdot P(r)$.
For the asymmetry analysis, the mean implied prevalence $\mathbb{E}[r_{implied}] = \sum_{r} r \cdot \hat{P}_{implied}(r \mid u)$ is compared to the expected endorsement prevalence $\mathbb{E}[r_{endorsed}] = \sum_{r} r\cdot \hat{P}_{endorsed}(r \mid u)$, where $\hat{P}_{endorsed}(r \mid u) \propto \hat{P}_{endorsed}(u \mid r) \cdot P(r)$.
By taking the mean of the prevalences at which a participant endorsed the generic, @Cimpian2010's analysis derives $\hat{P}_{endorsed}(r \mid u)$ by assuming a uniform prior on prevalences $P(r)$.
<!-- Recognizing the formal relationship behind @Cimpian2010's calculation makes explicit its assumptions (e.g., the uniform prior on $p$). -->

<!-- Because of the uniform prior, computing $P_{endorsed}(p \mid u)$ amounts to take the proportion of participants who endorse the generic at each prevalence level and normalizing those proportions to create a probability distribution.  -->
For example, let's say participants rate the item "Lorches are afraid of loud noises" as true at the five prevalence levels (10, 30, 50, 70, 90%) with proportions $[0.3, 0.3, 0.8, 0.9, 0.9]$, respectively (e.g., 30% of participants endorse the statement when 10% or 30% of lorches live in trees, 90% of participants endorse the statement when 70% or 90% live in trees, etc.).
These porportions are treated as an *unnormalized* likelihood distribution $\hat{P}_{endorsed}(u \mid r)$, multiplied by a uniform prior $P(r) = [1, 1, 1, 1, 1]$ to arrive at an unnormalized posterior of interest: $\hat{P}_{endorsed}(r \mid u)$. 
With a uniform prior on $P(r)$, the posterior $\hat{P}_{endorsed}(r \mid u)$ is the same as the likeihood $\hat{P}_{endorsed}(u \mid r)$ (up to normalization).
We normalize the posterior by dividing each by the sum of all endorsement proportions (e.g., $\frac{0.3}{0.3 + 0.3 + 0.8 + 0.9 + 0.9} = 0.09$, $\frac{0.9}{0.3 + 0.3 + 0.8 + 0.9 + 0.9} = 0.28$), yielding the following distribution: [0.09, 0.09, 0.25, 0.28, 0.28].
To get the average endorsed prevalence, we then take the expectation (i.e., the mean) of this distribution with respect to the prevalence level associated with each probability: *average endorsed prevalence* $= 0.09 \times 10\% + 0.09 \times 30\% + 0.25 \times 50\% + 0.28 \times 70\% + 0.28 \times 90\% \approx 60\%$.
The *average implied prevalence* is the mean of participants responses to the question: "How many lorches are afraid of loud noises?" after learning that "Lorches are afraid of loud noises" ($\sim 80\%$, from Ch. 2 Expt. 2). 
Thus, in this example, there is an asymmetry between the average endorsed prevalence ($60\%$) and average implied prevalence ($\sim 80\%$).

Average endorsed prevalence aims to measure the average conditions under which speakers would assert a generic statement. 
But by assuming a uniform prior on prevalence $P(r)$, @Cimpian2010 assume that speakers would encounter kinds with any prevalence level of a particular property equally as often. 
For example, the analysis assumes that kinds for which 100% have four legs is equally likely than kinds for which 30% have four legs. 
To more accurately model the situations under which a speaker would produce generics about kinds with various levels of property prevalence, we propose transforming endorsement judgments to the scale of endorsed prevalence, using a prior that encodes *need probabilities*, that is, the probability that a speaker would need to communicate about a kind with that level of prevalence [@Kemp2018].
For example, suppose that, in the actual environment, the true prevalence of being *afraid of loud noises* follows the following non-uniform distribution  [0.1, 0.05, 0.1, 0.1, 0.65], such that most animal kinds (65%) have high prevalence (90%) of the property, though there are plenty of kinds with lower prevalence.
If we were to use these need probabilities together with endorsement judgments to compute average endorsed prevalence,
<!-- As a proxy for a speaker's actual need probabilities, we can use participants' ratings of the prevalence of the feature in alternative categories, which we use as the listener's prevalence prior (Ch. 2, Expt. 2a).  -->
<!-- The need probabilities for different levels of prevalence of the property *are afraid of loud noises* (taken from Ch. 2, Expt. 2a) is non-uniform over different levels of prevalence: [0.18, 0.10, 0.15, 0.15, 0.42]. -->
<!-- That is, it is not equally as likely that a speaker would need to refer to a kind for which 30% were afraid of loud noises as it is to refer to a 90% afriad of loud noises kind.  -->
we would have:

\begin{align}
[0.3 \times 0.1, 0.3 \times 0.05, 0.8 \times 0.1, 0.9 \times 0.1, 0.9 \times 0.65] \\
\propto [0.04, 0.02, 0.10, 0.11, 0.73]
\end{align}

Using these need probabilities, the highest prevalence level has higher posterior probabilities of being endorsed because it is more likely to occur in the environment. 
The expected value of this distribution, the average endorsed prevalence, is 79.6%, not very different from the average implied prevalence of 80% for the same item.
The same pattern of endorsements with different assumptions about the environment can result in no asymmetry.



```{r}
schem.endorse <- c(0.3, 0.3, 0.8, 0.9,0.9)
schem.need <- c(0.1, 0.05, 0.1, 0.1, 0.65)
# schem.endorse * schem.need / sum((schem.endorse* schem.need))
post_probs <- schem.endorse * schem.need / sum((schem.endorse* schem.need))
#sum(post_probs*c(10, 30, 50, 70, 90))
```

<!-- Now that we have made explicit the mathematical relationship between implied and endorsed prevalence, we can apply the same analysis on a by-item, as opposed to a by-participant, basis.^[ -->
<!--   If one wanted to calculate average endorsed prevalence on a participant-wise basis in addition to an item-wise basis, each participant would have to make a judgment at every prevalence level for each item.  -->
<!--   This is perhaps pragmatically strange (though the task is already a bit pragmatically strange) and @Cimpian2010  opts against this.  -->
<!--   Instead, they calculate average endorsed prevalence at the participant level (intuitively, getting at each participant's threshold for endorsement) and aggregate at the level of *property type*  (e.g., biological vs. accidental) in order to compare domain differences. -->
<!-- ] -->

<!-- @Cimpian2010 argue that the asymmetry is anomalous when compared against quantifiers. -->
<!-- In theory, *all* should get the following endorsement pattern: $[0, 0, 0, 0, 1]$, which would result in an average endorsed prevalence of 100\%; similarly, if you learn that "All lorches live in trees", you ought to report that 100% live in trees. -->
<!-- *Most* ought to have a similar pattern:  $[0, 0, 1, 1, 1]$ for endorsements, resulting in an average endorsed prevalence of about 75%.^[ -->
<!--   We assume here that 50% prevalence is sufficient to endorse most. The theoretical symmetry between average endorsed prevalence and average implied prevalence does not depend upon this decision, but only that there is some threshold beyond which *most* is fully endorsed. -->
<!-- ] -->
<!-- If interpretation followed a fixed-threshold model with uniform priors on prevalence, the implied prevalence given a *most* statement would be uniformly distributed between 50% and 100%, resulting in an avaregae of 75%.  -->
<!-- Again, there is symmetry between these two measurements for *most* and *all*.  -->
<!-- However, this theoretical result for *most* assumes a uniform prior on $p$ for implied prevalence. -->
<!-- Should we expect something different for the generic? -->

We can analyze the expected behavior of the different ways of computing *average endorsed prevalence* using our computational model.
As a proxy for the need probability distribution, we use the listener's prevalence prior $P(r)$. 
This then assumes that the probability of various kinds having a property at some prevalence is a matter of what participants believe to be the case.
This will of course have problems for addressing questions of societal concern (e.g., propogation of stereotypes), but it is more realisitic than assuming a uniform distribution. 

From Chapters 2 and 3, we know that the following are good models for $P_{implied}(r \mid u)$ and $P_{endorsed}(u \mid r)$:

\begin{eqnarray}
L(r \mid u) \propto r \cdot P(r) \\
S(u \mid r) \propto L(r \mid u)^\alpha
\end{eqnarray}

<!-- We can take the need probability distribution $P_{endorse}(r)$ to be the same as the listener's prevalence prior distribution $P(r)$.  -->
<!-- #To compare to the theoretical analysis for *most*,  -->


<!-- Thus, the asymmetry for the average implied prevalence and average endorsed prevalence does not result from the semantics of generics itself.  -->

<!-- We next explore through simulation what happens when we do not assume a uniform prior on prevalence for the listener, and compare @Cimpian2010's method with our own. -->
Figure\ \@ref(fig:asymmetrySimulations) shows the predicted relationship between implied prevalence and endorsed prevalence for three different prevalence priors. 
Both implied prevalence and endorsement are sensitive the prior distribution over prevalence (Figure\ \@ref(fig:asymmetrySimulations)A-C). 
When average endorsed prevalence is computing assuming a uniform need probability [following @Cimpian2010], average endorsed prevalence  shows substantially reduced variability relative to average implied prevalence, producing asymmetries (Figure\ \@ref(fig:asymmetrySimulations)D).
However, if average endorsed prevalence is computed assuming a need probability distribution that follows the prevalence prior, the measure shows similar context-sensitivity as the implied prevalence, resulting in no substantial asymmetries (Figure\ \@ref(fig:asymmetrySimulations)E).

Computing averaged endorsed prevalence using a need probability distribution results in a derived measure with higher by-item variance.
This is a desirable property for a measurement we know to exhibit by-item variance, as evidence by the endorsement predictions (Figure\ \@ref(fig:asymmetrySimulations)C) assuming different prior distributions on prevalence (Figure\ \@ref(fig:asymmetrySimulations)A). 
Assuming a uniform need distribution, as done by @Cimpian2010, artificially reduces the context-sensitivity of *average endorsed prevalence* while maintaining it for the implied prevalence, potentially producing asymmetries where none exist.
We now turn to test whether an asymmetry still exists by calculating average endorsed prevalence assuming a need probability distribution that follows the prevalence prior.

<!-- We have already seen in Chapter 2 that the implied prevalence of a generic is highly sensitive to the prevalence prior distribution, and in Chapter 3 how endorsement can change as a function of the referent prevalence and the prevalence prior.  -->
<!-- In the following two experiments, we show how the *average endorsed prevalence* (which requires measuring endorsement at many prevalence levels) is less sensitive to the prevalence prior.  -->
<!-- We also how our pair of computational models (endorsement and interpretation) can be used to jointly model data from both experimental tasks simultaneously. -->


<!-- \mht{did Anthea or others do context-sensitive most interpretations?} -->
<!-- However, *most* can receive context-sensitive interpretations: "Most Americans voted for Hilary Clinton" means roughly 52%, whereas "Most dogs have four legs" means 99%.  -->


```{r asymmetrySimulations, fig.width=11, fig.asp=0.6, cache=F, fig.cap="Model simulations. A: Three theoretically interesting prior distributions over prevalence. B: Generic interpretation (implied prevalence) distributions. C: Endorsement model predictions given different referent prevalence levels and different prevalence priors. D-E: Average endorsed prevalence vs. average implied prevalence. D: Average endorsed prevalence computing assuming a uniform need probability (following @Cimpian2010). E: Average endorsed prevalence computing assumping a need probability distribution that follows the property-specific prevalence prior."}
load(file = "cached_results/modelSims-priors_fixedT_uncertainT.RData")
get.colors <- function(pal) brewer.pal(brewer.pal.info[pal, "maxcolors"], pal)
spectrum.color.palette <- get.colors("Blues")
load("cached_results/modelSims-asymmetry.RData")
load("cached_results/modelSims-asymmetry_needProbs.RData") # m.asym.need_probs
fig.sims.priors <- ggplot(sims.combined %>%
         filter(
           PriorShape %in% c("uniform", "biological_rare", "accidental_rare"),
           src %in% c("priors")
           ) %>%
         mutate(PriorShape = factor(PriorShape, levels = c("uniform", "biological_rare", "accidental_rare"),
                                    labels = c("uniform", "rare, strong", "rare, weak")),
                src = factor(src, levels = c( "priors"),
                             labels = c(
                               'prevalence prior'
                                        ))), 
       aes(x = state, color = PriorShape))+
    geom_density(aes(y = ..scaled..), fill = 'grey32',size = 0.6, alpha = 0.7, adjust = 1.1)+
    theme_few() +
    scale_color_brewer(palette = 'Set1') + 
    guides(color = F)+
    scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    ylab("Probability density") +
    xlab("Prior Prevalence")+
    #scale_color_solarized()+
    #scale_fill_solarized()+
    facet_grid(src~PriorShape, scales = 'free')+
    theme(
          legend.position = "none",
          strip.text.y = element_blank()
          )


fig.sims.distributions <- ggplot(sims.combined %>%
         filter(
           PriorShape %in% c("uniform", "biological_rare", "accidental_rare"),
           src == "posteriors"
           #src %in% c("fixed_0.1", "fixed_0.33", "fixed_0.5", "posteriors")
           ) %>%
         mutate(PriorShape = factor(PriorShape, levels = c("uniform", "biological_rare", "accidental_rare"),
                                    labels = c("Xs Y\n[uniform]", "Xs fly\n[biological]", "Xs carry malaria \n [accidental]")),
                src = factor(src, levels = c("posteriors"),
                             labels = c('generic interpretation'))),
                # src = factor(src, levels = c( "fixed_0.1", "fixed_0.33",
                #                               "fixed_0.5", "posteriors"),
                #              labels = c(
                #            #    'prevalence prior',
                #                '"some"\n(threshold = 0.01)',
                #                '"more than a third"\n(threshold = 0.33)',
                #               '"most"\n(threshold = 0.5)',
                #                'generic\n(uncertain threshold)'
                #                         ))), 
       aes(x = state, fill = PriorShape, color = src))+
    geom_density(aes(y = ..scaled..), 
                 color ='black', size = 0.6, alpha = 0.7, adjust = 1.1)+
    theme_few() +
    scale_fill_brewer(palette = 'Set1') + 
    guides(fill = F)+
    scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    ylab("Probability density") +
    xlab("Implied Prevalence")+
    # scale_fill_brewer(palette=2, type = "seq", direction = -1)+
    # scale_color_brewer(palette=2, type = "seq", direction = -1)+
    #scale_color_manual( values =  c(spectrum.color.palette[c(8,5,2)], "#238b45"))+
    #scale_fill_manual( values = c(spectrum.color.palette[c(8,5,2)], "#238b45") )+
    facet_grid(src~PriorShape, scales = 'free')+
    theme(strip.text.y = element_blank(),# element_text(angle = 0, size = 12),
          legend.position = "none")

fig.sims.truthJudgments <- s1.simulations %>%
  filter(model == "generic") %>%
  mutate(prior = factor(prior, levels = c("uniform", "biological_rare", "accidental_rare"),
                                    labels = c("uniform", "rare, strong", "rare, weak"))) %>%
  ggplot(., aes ( x = referent_prevalence, y = endorsement,
                  linetype = prior, color = prior))+
    scale_color_brewer(palette = 'Set1') + 
  geom_point()+
  geom_line()+
  theme(legend.position = c(0.8, 0.4) ,
        legend.title = element_blank())+
    scale_x_continuous(breaks = c(0, 0.5, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 0.5, 1), limits= c(0, 1))+
  ylab("Endorsement probability")+
  xlab("Referent prevalence")


fig.sims.asymmetry <- m.gen.asym %>%
  filter(src == "posteriors") %>%
  mutate(src = factor(src),
         PriorShape = factor(PriorShape, levels = c( "uniform",  "biological_rare", "accidental_rare"),
                                    labels = c( "uniform", "rare, strong",  "rare, weak")),
         model = factor(model, levels = c("S1", "L0"), labels = c("Endorsed", "Implied"))) %>%
  ggplot(., aes(x = PriorShape, y = expval, 
                fill = PriorShape, alpha = model))+
  scale_fill_brewer(palette = 'Set1') + 
  scale_alpha_manual(values = c(0.6, 1))+
      scale_y_continuous(breaks = c(0, 0.5, 1), limits= c(0, 1))+
  ylab("Average prevalence")+
  geom_col(position= position_dodge(), color = 'black')+
  #geom_errorbar(position = position_dodge())+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
        legend.position = c(0.80, 0.7),
        legend.title = element_blank(),
        axis.title.x = element_blank())+
  guides(fill = F, alpha =F)+
  ggtitle("Uniform priors")



fig.sims.asymmetry_needProbs <- m.asym.need_probs %>%
  mutate(PriorShape = factor(PriorShape, levels = c( "uniform",  "biological_rare", "accidental_rare"),
                                    labels = c( "uniform", "rare, strong",  "rare, weak")),
         model = factor(src, levels = c("S1", "L0"), labels = c("Endorsed", "Implied"))) %>%
  ggplot(., aes(x = PriorShape, y = expval, 
                fill = PriorShape, alpha = model))+
  scale_fill_brewer(palette = 'Set1') + 
  scale_alpha_manual(values = c(0.6, 1))+
      scale_y_continuous(breaks = c(0, 0.5, 1), limits= c(0, 1))+
  ylab("Average prevalence")+
  geom_col(position= position_dodge(), color = 'black')+
  #geom_errorbar(position = position_dodge())+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
        legend.position = c(0.80, 0.7),
        legend.title = element_blank(),
        axis.title.x = element_blank())+
  guides(fill = F)+
  ggtitle("Need priors")


cowplot::plot_grid(
  cowplot::plot_grid(
    fig.sims.priors + theme(plot.margin = unit(c(6, 6, 6, 18), "pt")), 
    fig.sims.distributions + theme(plot.margin = unit(c(6, 6, 6, 18), "pt")),
    fig.sims.truthJudgments + theme(plot.margin = unit(c(6, 6, 6, 18), "pt")),
    ncol = 1,
    labels = c("A", "B", "C")
  ),
  cowplot::plot_grid(
    fig.sims.asymmetry + theme(plot.margin = unit(c(18, 6, 6, 6), "pt")),
    fig.sims.asymmetry_needProbs + theme(plot.margin = unit(c(6, 6, 6, 6), "pt")),
    ncol = 1,
    labels = c("D","E"),
    rel_heights  = c(1, 1)
    ),
  #fig.sims.bars+ theme(plot.margin = unit(c(6, 0, 0, 0), "pt")),
  nrow = 1,
  labels = c("", ""),
  rel_widths = c(0.6, 0.45)
)

```


<!-- @Cimpian2010 found that the average implied prevalence for novel *most* statements about animals was also roughly 75%.  -->
<!-- These average implied prevalence ratings are consistent  -->








<!-- *All* is the easiest to think about: On a true/false task, participants should only endorse an *all* quantified statement when the prevalence is 100%; separately, when participants learn from a sentence containing *all*, they should believe the prevalence is 100%.  -->
<!-- From a simplified analysis, the quantifier *most* is true when the prevalence is greater than 50% (i.e., *most* means roughly *more than half*); the average prevalence at which a person will assent to a statement involving *most* will be 75% (i.e., all prevalences between 50-100%). -->
<!-- Given a uniform prior over the prevalence, a literal listener will interpret *most* as being consistent will all prevalences greater than 50%; again, averaging over these possilibities, the mean implied prevalence will be 75%.^[ -->
<!--   The lack of the asymmetry for *most* was shown in @Cimpian2010 Expt. 1. -->
<!-- ] -->
<!-- On a literal analysis, *some* behaves similarly: It rules out prevalence of 0%, and leaves open all other possibilities; on a literal analysis, the average truth conditions and the mean implied prevalence should be about 50%. -->


<!-- However, because participants in @Cimpian2010's experiment rate generics about each property only once (with only one prevalence level; e.g., 30% have yellow fur), this analysis collapses across generics of individual items used in the experiment. -->
<!-- Unless a participant provides judgments at all prevalence levels (10, 30, 50, 70, 90%) for a particular item (e.g., have yellow fur), then the average endorsed prevalence for a particular item cannot be computed.  -->
<!-- Thus, we propose a similar, alternative formulation of averaged endorsed prevalence derived from basic probability theory.  -->
<!-- Our formulation will generalize to item-wise analysis we explore in the empirical part of this paper. -->


<!-- An asymmetry between truth conditions and interpretations occurs when the prevalence levels at which a statement is endorsed differ from the prevalences implied by the same statement.  -->
<!-- The asymmetry is present generic statements about biological properties, as reported by @Cimpian2010. -->
<!-- Generic statements are accepted for a wide range of prevalence levels (e.g., even when the property is present in only 10% of cases) but when participants learn from generics, the statements are often interpreted quite strongly (e.g., all or almost all have the property).  -->

<!-- ## Technical description -->

<!-- The empirical asymmetry between truth conditions and interpretations was reported by @Cimpian2010. -->
<!-- Truth conditions data result from an endorsement task, a two-alternative forced choice (2AFC) paradigm (as explored in Chapter 3). -->
<!-- Interpretation data are prevalence ratings, typically on a 101-pt scale (e.g., the percentage of the category with the property). -->
<!-- Thus, these two constructs are not in the same measurement space; one must be transformed into the other in order to compare them. -->



<!-- ## Criticisms of the asymmetry analysis -->

<!-- Translating this theoretical analysis into predictions about observed behavior is less than straightforward.^[ -->
<!--   It is not my goal to delve too deep into the intracacies of comparing truth judgments with prevalence ratings, this being part of more broadly a comparison between speaker and listener behavior.  -->
<!--   I will not present models of other quantifiers and show quantifiers-by-domain interactions, though this is clearly an area ripe for modeling.  -->
<!--   Instead, I will try to highlight some of the issues, as I see it, that will help us better understand the comparison of truth judgments and implied prevalence ratings for generic statements, specifically.  -->
<!-- ] -->
<!-- I believe literal endorsement tasks (as the *truth conditions* task purports to be) are possible.  -->
<!-- In @Yoon2016 and @Yoon2017, we attempted to empirically measure "literal semantics" by providing participants with both a state of the world and an utterance and asking the participant if they thought that the speaker thought the utterance was true (e.g., "John baked a cake. Sally thought the cake deserved a rating of 5 out of 5. Do you think Sally thought John's cake was good?"). -->
<!-- Here we found no effects of pragmatic production behavior (e.g., participants rated the adjective "good" as equally good for 4 out of 5 and for 5 out of 5, even the adjective "amazing" was also present on different trials in the experimental context). -->
<!-- The setup of very similar to the truth conditions task of @Cimpian2010 wherein participants are supplied with both a state of the world (here, prevalence) and an utterance (a generic) and asked if they thought it was true or false. -->
<!-- However, @Cimpian2010 Expt. 2 show results for the truth conditions task using the quantifier "some" and find less than full endorsement for "some" at 70% and 90% prevalence.  -->

<!-- Attempting a literal interpretation task is even trickier.  -->
<!-- Language understanding is in general a pragmatic process which should take into account both a listener's prior beliefs and their assumptions of the speaker. -->
<!-- Therefore, in an implied prevalence task involving *some* (e.g., "Some lorches have purple feathers"), we would expect participants to report a prevalence consisntent with *some but not all* (perhaps even *some and not most*). Additionally, prior beliefs can rarely be assumed to be uniform.  -->
<!-- In our empirical work thus far, we have seen how prevalence prior distributions vary dramatically across different kinds of properties and we already know (from Chapter 2) that these influence participants' implied prevalence ratings. -->


<!-- The above analysis assumes uniform priors over prevalence and a literal interpretation model (on the implied prevalence side) as well as a model of a literal speaker (on the truth conditions side).  -->
<!-- Were any of these assumptions to prove false, the theoretical symmetries would break.  -->
<!-- @Cimpian's data is from language experiments, for which pragmatic understanding should be the default assumption.  -->
<!-- It is possible the truth conditions task should also be thought of as pragmatic. -->
<!-- @Degen's gumballs show that participants are reluctant to endorse a *some* statement when *all* is present. -->
<!-- For example, the implied prevalence task can easily be construed as a pragmatic  -->


<!-- 
Technical aspects of the asymmetry:
  - Theoretical exploration using "some", "most" as simply threshold-functions (no speaker) and Uniform priors for the interpretation model
    - This will show the symmetry for both?
    - What will it show for the generic? (what about an L1?)
  - For some, most: interpretation is potentially more complicated (some has the not all [not most?] implicature, which Cimpian finds... "most" maybe as well?)
  - The dynamic range for the asymmetry is more limited for ATC than IP, so must of the action happens because of IP
  
  - Cimpian did it subject-wise but not property-wise (assuming property classes and equivalence within a class...). How do you do it propery-wise?
    - [ ] do you get similar results by bootstrapping?
    - [ ] can you build a bayesian model of these data?
    - one possibility is to have subjects rate all prevalence levels... but this may be strange for people
    - another is to share statistical strength
      - assume there is a threshold for each property
      - each participant's threshold is a sample from this distribution...
      - but properties are completely independent? still may be too sparse
        - maybe you don't need the participant level: just have a threshold model that has the possibility of noise?
-->


<!-- - Model figure: -->
<!--   - Prior1, L("Generic1"), S: x=%, y= endorse, group = prior,  ATC & IP: bar plot (x = model) -->
<!--   - Prior2, L("Generic2"), S: x=%, y= endorse,  ATC & IP: bar plot (x = model) -->
<!--   - Prior3, L("Generic3"), S: x=%, y= endorse,  ATC & IP: bar plot (x = model) -->

<!-- 
Computing ATC for model should just be the mean of the production probabilities...
This probably is the same as the Cimpian sampling method, though won't have a way of constructing the variance..
thus the sampling method may be preferred for constructing the variance
-->


# Experiment 1: Replication and extension of Cimpian et al. (2010) truth conditions task

In Chapter 2, we replicated @Cimpian2010's implied prevalence paradigm using a slightly expanded stimulus set (Expt. 1B). 
We now measure endorsements using @Cimpian2010's *truth conditions* paradigm.
Here, we replicate the truth conditions task using the slightly expanded stimulus set used in Ch. 2.
The relevant prevalence priors have already been measured in Chapter 2 (Expt. 1A).

## Method

### Participants

We recruited 40 participants over MTurk.
We chose a sample size at least twice as large as the original study by @Cimpian2010 (original $n = 20$), and to match that of Ch. 2 Expt. 1B. 
All participants were self-reported native English speakers and none completed the analagous implied prevalence task (Ch. 2 Expt. 1B).
The experiment took about 5 minutes and participants were compensated \$0.60.

### Procedure and materials

The cover story and materials were the same as in Ch. 2, Expt. 1B and the procedure followed that of @Cimpian2010 Expt. 1.
On each trial, participants were given a statement about a property's prevalence within a novel kind (*referent prevalence*; e.g. "50% of feps have yellow fur."). 
Participants were then asked whether or not they agreed or disagreed with the corresponding generic sentence (e.g. "Feps have yellow fur."). 
Referent prevalence varied between 10, 30, 50, 70, and 90%.

The experiment consisted of 25 trials: 5 trials for each of 5 types of properties measured in Ch. 2, Expt. 1A (part, color part, vague part, common accidental, rare accidental). 
Each prevalence level appeared once for each property type (5 prevalence levels x 5 property types). 

### Results



```{r figCbgResults, fig.width=11, fig.asp = 0.75, fig.cap="Proportion of endorsements for novel generic sentences in Expt. 1 (Replication of @Cimpian2010 Expt. 1)."}

load("../../generic-interpretation/paper/cached_results/cimpian-prevPrior-interpretations.Rdata")
# m.rs.priors.reconstructed.subset,
     # df.c.prior.bs.wide, 
     # df.c.int.bs,
load(file = "cached_results/cbg_results.RData")
load("cached_results/cbg_results_aedNeed.RData") # df.c.need_aed.int.bs, df.c.need_aed.int.item.bs.summary,df.c.unif_aed.int.item.bs.summary

fig.prevPriors <- ggplot(m.rs.priors.reconstructed.subset %>%
                           filter(property %in% c("claws", "big claws", "blue claws", "worn-out claws")), 
       aes( x = prevalence,
                                    fill = stim_type))+
    stat_density ( aes(y = ..scaled.. ), color = 'black')+
    facet_wrap(~property, nrow = 2)+
    scale_x_continuous(limits = c(-0.01,1.01), breaks = c(0, 1)) +
    scale_y_continuous(limits = c(-0.01,1.01), breaks = c(0, 1)) +
    theme(strip.text.y = element_text(angle = 0))+   
  scale_fill_solarized()+
  scale_color_solarized()+
  guides(fill=F)+
  ylab("Probability density (scaled)")+
  xlab("Prevalence")


dodge_width <- 7

fig.cbg.tj <- df.c.endorse_by_prev %>%
  mutate("Property type" = factor(stim_type,
                                         levels = c("part",
                                                    "vague",
                                                    "color",
                                                    "accidental"),
                                         labels = c("body part",
                                                    "color adj + part",
                                                    "gradable adj + part",
                                                    "accidental"))) %>%
  ggplot(., aes( x = stim_prevalence, y = mean, ymin = ci_lower,
                 ymax = ci_upper, fill = `Property type`,
                 color = `Property type`))+
  geom_line(position = position_dodge(dodge_width), alpha = 0.4,
            linetype = 3)+
  geom_linerange(position = position_dodge(dodge_width), alpha = 0.6,
                 size = 1)+
  geom_point(position = position_dodge(dodge_width),
             size = 2.5, shape = 21, color = 'black')+
  scale_fill_solarized()+
  scale_color_solarized()+
  ylab("Proportion endorsement")+
  xlab("\n Referent prevalence")+
  scale_x_continuous(limits = c(0, 100), breaks = c(10,  30, 50, 70,90))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  theme(legend.position = c(0.75, 0.27))+
  coord_fixed(ratio = 65)

dodge_width <- 0.9

fig.cbg.asym <- bind_rows(
  df.c.int.bs %>% mutate(src = 'implied prevalence'),
  df.atc %>% mutate(src = 'truth conditions')
) %>%
  mutate("Property type" = factor(stim_type,
                                         levels = c("part",
                                                    "vague",
                                                    "color",
                                                    "accidental"),
                                         labels = c("body part",
                                                    "color adj + part",
                                                    "gradable adj + part",
                                                    "accidental")),
         src = factor(src, levels = c("truth conditions",
                                      "implied prevalence"),
                      labels = c("Endorsed", "Implied"))) %>%
  ggplot(., aes( x = `Property type`, y = mean, ymin = ci_lower,
                 ymax = ci_upper, fill = `Property type`, alpha = src))+
  geom_col(position = position_dodge(dodge_width), width = dodge_width,
           color = 'black')+
  geom_errorbar(position = position_dodge(dodge_width),
                 size = 1, width = 0.1)+
  scale_fill_solarized()+
  scale_alpha_manual(values = c(0.6, 1))+
  scale_y_continuous(limits = c(0, 100))+
  ylab("Average prevalence")+
  ggtitle("Uniform priors")+
  guides(fill = F, alpha =F)+
  theme(axis.text.x = element_text(angle = 45, vjust =1 ,hjust = 1),
        legend.title = element_blank(),
        axis.title.x = element_blank()#,
        #legend.position = c(0.75, 0.9)
        )




fig.cbg.asym.need <- df.c.need_aed.int.bs %>%
  mutate("Property type" = factor(stim_type,
                                         levels = c("part",
                                                    "vague",
                                                    "color",
                                                    "accidental"),
                                         labels = c("body part",
                                                    "color adj + part",
                                                    "gradable adj + part",
                                                    "accidental")),
         src = factor(src, levels = c("truth conditions",
                                      "implied prevalence"),
                      labels = c("Endorsed", "Implied"))) %>%
  ggplot(., aes( x = `Property type`, y = mean, ymin = ci_lower,
                 ymax = ci_upper, fill = `Property type`, alpha = src))+
  geom_col(position = position_dodge(dodge_width), width = dodge_width,
           color = 'black')+
  geom_errorbar(position = position_dodge(dodge_width),
                 size = 1, width = 0.1)+
  scale_fill_solarized()+
  scale_alpha_manual(values = c(0.6, 1))+
  scale_y_continuous(limits = c(0, 100))+
  ylab("Average prevalence")+
  ggtitle("Need priors")+
  guides(fill = F)+
  theme(axis.text.x = element_text(angle = 45, vjust =1 ,hjust = 1),
        legend.title = element_blank(),
        legend.position = c(0.83, 0.8),
        axis.title.x = element_blank())


cowplot::plot_grid(
  cowplot::plot_grid(
    fig.cbg.tj + theme(plot.margin = unit(c(6, 3, 30, 0), "pt")), 
    fig.prevPriors + theme(plot.margin = unit(c(6, 3, 30, 0), "pt")), 
    nrow = 1,
    labels = c("A", "B"),
    rel_widths = c(1.3,1)
  ),
  cowplot::plot_grid(
    fig.cbg.asym + theme(plot.margin = unit(c(6, 6, 6, 6), "pt")),
    fig.cbg.asym.need + theme(plot.margin = unit(c(6, 6, 6, 6), "pt")),
    nrow = 1,
    labels = c("C", "D"),
    rel_widths = c(1,1)
  ),
  ncol = 1,
  rel_heights = c(1,1)
)

```


```{r cbg results brms}
load("cached_results/cbg_results_brms.RData") #rs.brm.endorse.prev.type

rs.brm.endorse.prev.type.summary <- summary(rs.brm.endorse.prev.type)
```


Figure\ \@ref(fig:figCbgResults)A show the proportion of endorsements at each level of prevalence for the four different kinds of properties.
To estimate any effects of property type, we ran a Bayesian bernoulli mixed-effects regression model with fixed effects of prevalence (centered and scaled), property type and their interaction and random effects of intercept and property type by-participants and intercept by-item.
There is a clear effect of prevalence levels $\beta_{prevalence} =$ `r format_regression_effects(rs.brm.endorse.prev.type.summary, "stim_prevalence")`, and some evidence that generics with body part properties $\beta_{body} =$ `r format_regression_effects(rs.brm.endorse.prev.type.summary, "stim_typepart")` and those described with gradable adjectives $\beta_{gradable} =$ `r format_regression_effects(rs.brm.endorse.prev.type.summary, "stim_typevague")` are endorsed more than the accidental property bare plurals. 
In addition, there is evidence for an interaction between prevalence and property type for the body $\beta_{body \times prevalence} =$ `r format_regression_effects(rs.brm.endorse.prev.type.summary, "stim_prevalence:stim_typepart")` and gradable properties $\beta_{gradable \times prevalence} =$ `r format_regression_effects(rs.brm.endorse.prev.type.summary, "stim_prevalence:stim_typevague")` in comparison to the accidental properties: The slope appears flatter for the accidental bare plurals, owing to the fact they are endorsed less at high prevalence levels.
These same effects were not credibly different from zero for the color properties ($\beta_{color} =$ `r format_regression_effects(rs.brm.endorse.prev.type.summary, "stim_typecolor")`; $\beta_{color \times prevalence} =$ `r format_regression_effects(rs.brm.endorse.prev.type.summary, "stim_prevalence:stim_typecolor")`).

Figure\ \@ref(fig:figCbgResults)B shows four example prevalence priors (from Ch. 2, Expt. 1a) used as need distributions for computing average endorsed prevalence. 
Figure\ \@ref(fig:figCbgResults)C shows the average endorsed prevalence, computed using the procedure by @Cimpian2010 (assuming a uniform prior), in comparison to the average implied prevalence from the same items (from, Ch. 2 Expt 1b).
We see that the *averaged endorsed prevalence* measure does vary by property-type, owing to roughly similar endorsement pattern in Figure\ \@ref(fig:figCbgResults)A. 
As we saw in our simulations, however, even were the endorsements to vary more dramatically, the resulting influence on this measure of average endorsed prevalence would be relatively minimal, because of the averaging step in its calculation. 
Thus, the asymmetry reported by @Cimpian2010 is driven by the implied prevalence varying between property (a phenomenon we explored in depth in Ch. 2) while the average endorsed prevalence does not change.

For *average endorsed prevalence* to roughly correspond to the environmental statistics under which we would expect generics to be used about kinds with various levels of property prevalence, we should not only take into account the endorsement patterns at face value but weight them by the prior probability of kinds being present in the enviroment to be referred to.
Figure\ \@ref(fig:figCbgResults)D shows the average endorsed prevalence, computed using the need distributions.
This average endorsement measure takes into account that different properties occur with different frequencies in kinds, and displays context-sensitivity that is comparable to that of the implied prevalence.
Notably, however, we still see that the prevalence implied by generics of the body part properties, and possibly the other non-accidental properties, still reach higher levels than the average endorsed prevalence. 
This would be potentially much stronger evidence than @Cimpian2010 that certain generics are interpreted more strongly than the endorsement patterns would suggest.

To examine in more detail the relationship by the need-probability derived average endorsed prevalence and implied prevalence, we compare the two measures on a by-item basis assuming both the uniform prior of @Cimpian2010 and our prevalence need prior. 
Figure\ \@ref(fig:cbg_aid_aed_byItem) shows the results. 
It is clear that the uniform prior assumption substantially decreases the dynamic range of the endorsement data. Across the forty items, the average implied prevalence ranges from 40% to 95% (55% range).
The endorsed prevalence assuming a uniform prior only ranges from 53% to 75% (22% range), whereas assuming our need prior ranges from 46% to 84% (38% range).
The uniform endorsed prevalence shows almost no correlation with the implied prevalence data ($r(40) = 0.13$), whereas the need prior derived endorsed prevalence shows a very high correlation $r(40) = 0.81$.
This is because the prevalence implied by generics is highly correlated with the prevalences under which we would expect the same generics to be produced.

```{r eval =F}
with(df.c.unif_aed.int.item.bs.summary, range(mean))

with(df.c.unif_aed.int.item.bs.summary, range(aed_mean))
with(df.c.need_aed.int.item.bs.summary, range(aed_mean))

with(df.c.unif_aed.int.item.bs.summary, cor(aed_mean, mean))
with(df.c.need_aed.int.item.bs.summary, cor(aed_mean, mean))
```


```{r cbgAidAedByItem, fig.width=6, fig.asp = 0.6, out.width="80%", fig.cap="Comparison of average endorsed prevalence and average implied prevalence assuming a uniform need prior (left) and a property-specific need prior (right)."}
bind_rows(
  df.c.unif_aed.int.item.bs.summary %>% mutate(src = 'uniform prior'),
  df.c.need_aed.int.item.bs.summary %>% mutate(src = 'need prior')
) %>%
  mutate(src = factor(src, levels = c('uniform prior', 'need prior')),
         "Property type" = factor(stim_type,
                                         levels = c("part",
                                                    "vague",
                                                    "color",
                                                    "accidental"),
                                         labels = c("body part",
                                                    "color adj + part",
                                                    "gradable adj + part",
                                                    "accidental"))) %>%
  ggplot(., aes( x = aed_mean, xmin = aed_lower, 
                 xmax = aed_upper,
                 y = mean, ymin = ci_lower,
                 ymax = ci_upper, color = `Property type`))+
  geom_abline(intercept = 0, slope = 1, lty = 3)+
  geom_errorbar(alpha = 0.15)+
  geom_errorbarh(alpha = 0.15)+
  geom_point(size = 2)+
  scale_color_solarized()+
  scale_y_continuous(limits = c(25, 100))+
  scale_x_continuous(limits = c(25, 100))+
  coord_fixed()+
  facet_wrap(~src)+
  #ylab("Average truth conditions")+
  guides(fill = F)+
  xlab("average endorsed prevalence")+
  ylab("average implied prevalence")
```



<!-- We see that any differences that may exist for truth judgments at the various prevalence levels disappear when the *average endorsed prevalence* is computed.  -->
<!-- This statisticla signature, combined with the highly property-sensitive *implied prevalence*, produces asymmetries to varying degrees.  -->

<!-- We generate model predictions by using the endorsement model to predict the truth judgment data using the parameters fit to the interpretation data from Ch. 2 (Expt. 1).  -->
<!-- That is, these predictions are not fit to the empirical truth judgment data, but generated *a priori* from the model.  -->
<!-- The endorsement model does have a free parameter, however, the speaker optimality parameter $\alpha$.  -->
<!-- We set $\alpha = 1$ for these simulations.  -->


```{r cbgModelResults, fig.width=10.5, fig.asp = 0.35, fig.cap="Model predictions for Expt. 1.", eval = F}
load(file = "cached_results/cbg_modelpredictions_alpha1_apriori.RData") #m.cbg.asym, m.c.asym.end, 

dodge_width <- 0.07

fig.cbg.m.tj <- m.c.asym.end %>%
  mutate("Property type" = factor(param,
                                         levels = c("part",
                                                    "vague",
                                                    "color",
                                                    "accidental"),
                                         labels = c("body part",
                                                    "color adj + part",
                                                    "gradable adj + part",
                                                    "accidental"))) %>%
  ggplot(., aes(  x = as.numeric(category), y = MAP, ymin = cred_lower,
                 ymax = cred_upper, fill = `Property type`,
                 color = `Property type`))+
  geom_line(position = position_dodge(dodge_width), alpha = 0.4,
            linetype = 3)+
  geom_linerange(position = position_dodge(dodge_width), alpha = 0.6,
                 size = 1)+
  geom_point(position = position_dodge(dodge_width),
             size = 2.5, shape = 21, color = 'black')+
  scale_fill_solarized()+
  scale_color_solarized()+
  ylab("Proportion endorse")+
  xlab("\n Referent prevalence")+
  scale_x_continuous(limits = c(0, 1), breaks = c(.10,  .30, .50, .70, .90))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  theme(legend.position = c(0.75, 0.27),
        legend.title = element_blank())#+
  #coord_fixed(ratio = 65)

dodge_width <- 0.9

fig.cbg.asym.m <- m.cbg.asym  %>%
  ungroup() %>%
  mutate("Property type" = factor(param,
                                         levels = c("part",
                                                    "vague",
                                                    "color",
                                                    "accidental"),
                                         labels = c("body part",
                                                    "color adj + part",
                                                    "gradable adj + part",
                                                    "accidental"))) %>%
  ggplot(., aes( x = `Property type`, y = MAP, ymin = cred_lower,
                 ymax = cred_upper, fill = `Property type`, alpha = src))+
  geom_col(position = position_dodge(dodge_width), width = dodge_width,
           color = 'black')+
  geom_errorbar(position = position_dodge(dodge_width),
                 size = 1, width = 0.1)+
  scale_fill_solarized()+
  scale_alpha_manual(values = c(0.6, 1))+
  scale_y_continuous(limits = c(0, 1))+
  ylab("Average prevalence")+
  guides(fill = F)+
  theme(axis.text.x = element_text(angle = 45, vjust =1 ,hjust = 1),
        legend.title = element_blank(),
        axis.title.x = element_blank()#,
        #legend.position = c(0.75, 0.9)
        )

cowplot::plot_grid(
  fig.cbg.m.tj + theme(plot.margin = unit(c(6, 3, 30, 0), "pt")), 
  fig.cbg.asym.m + theme(plot.margin = unit(c(6, 6, 6, 0), "pt")),
  #fig.sims.bars+ theme(plot.margin = unit(c(6, 0, 0, 0), "pt")),
  nrow = 1,
  labels = c("A", "B"),
  rel_widths = c(1,1)
)
```

<!-- the interpretation model to predict the implied prevalence data (as well as the simple statistical model to fit the prevalence prior data, both of which are from Ch. 2 Expt. 1). -->
<!-- That is, a single model with the same prior knowledge is required to fit three data sources simultaneously.  -->
<!-- The model parameters were inferred using Bayesian inference, putting the following prior on the speaker optimality parameter $\alpha \sim \text{Uniform}(0, 10)$. -->

<!-- Figure\ \@ref(fig:cbgModelResults)A shows the predicted truth judgment data at each prevalence level. -->
<!-- We see that the model shows increasing endorsements as a function prevalence.  -->
<!-- Contra the empirical data, however, there is a small tendency to endorse the accidental generics *more*. -->
<!-- Figure\ \@ref(fig:cbgModelResults)B shows the average endorsed prevalence in comparison to the average implied prevalence. -->
<!-- We see two tendencies consistent with the empirical data. -->
<!-- Average endorsed prevalence is less sensitive to the kind of property than the average implied prevalence, and the asymmetry quantitatively decreases and disappears in a way analagous to the empirical data. -->
<!-- However, the model predictions for the accidental generics does not predict the asymmetry reversal we observe in the empirical data. -->
<!-- This may be a function of the predicted increased endorsements for the accidental generics, leading to a lower average endorsed prevalence. -->

<!-- In sum, the model predicts that the average endorsed prevalence will be less sensitive to the property than the average implied prevalence and predicts that the asymmetry will disappear for certain properties. -->
<!-- However, the model does a poor job at predicting the endorsement data at each prevalence level for the generics about accidental properties.  -->
<!-- This is because the observed endorsement pattern is inconsistent with the implied prevalence data.  -->
<!-- That is, the prevalence priors for accidental generics are skewed towards low prevalence levels, analagous to the *rare weak* priors in Figure\ \@ref(fig:asymmetrySimulations). -->
<!-- Thus, the truth conditions ought to be more relaxed, analagous to a case like "Mosquitos carry malaria".  -->
<!-- One possibility is that participants believe the *predictive prevalence* of the accidental features is lower than the prevalence stated in the prompt (e.g., 30% of lorches have broken legs today, but in the future, this number should be lower), analagous to the case of extrinsically acquired features we saw in Chapter 4 (Expt 1).  -->
<!-- Trying to accomodate the fine-grained differences for the bare plurals about accidental properties (e.g., "Lorches have broken legs") would lead us too far astray, however. -->
<!-- We have accomplished our primary goal: Showing that the average endorsed prevalence is much less sensitive to the property relative to the average implied prevalence. -->




<!-- As can be gleaned from the figure, this is a result of the implied prevalence predicted to be *too high*.  -->
<!-- This contrasts with the model predictions from Ch. 2, Expt. 1, where the predicted implied prevalence tracked the empirical implied prevalence to a high degree. -->
<!-- This mismatch is a result of the single model trying to account for both endorsements and interpretations simultaneously, and the model making the wrong prediction for endorsements. -->
<!-- and the implied prevalence data.  -->


<!-- ### Extended analysis: Average endorsed prevalence -->

<!-- 
  - [1.] do you get similar results by bootstrapping subjects and not going straight to ATC (property type level)?
  - [2.] if so, can you do bootstrapping at the individual property level?
  - [3.] can you build a bayesian model of these data? 
-->
    
# Experiment 2: Property-specific asymmetries

In this experiment, we measure *average endorsed prevalence* for the larger and more diverse stimulus set (same as used in Ch. 2, Expt. 2).
The goal is to estimate the variability in average endorsed prevalence for items we know have high variability in average implied prevalence.

## Method

### Participants

We recruited 250 participants from MTurk. 
This number was arrived at with the intention of getting approximately 50 ratings for each unique item in the experiment.
The experiment took on average 4 minutes and participants were compensated \$0.75.
The primary analysis, sample size, and exclusion criteria were preregistered: https://osf.io/wvuj2/register/5771ca429ad5a1020de2872e

### Materials

We chose 25 properties from the larger stimulus set of 75 properties used in Ch. 2, Expt. 2.
The 25 properties were chosen by ordering the 75 properties in terms of increasing implied prevalence ratings, and selecting every third property.
<!-- Items were generated by considering six different classes of properties: physical characteristics (e.g., *have brown spots*), psychological characteristics (e.g., *experience emotions*), dietary habits (e.g., *eat human food*), habitat (e.g., *live in zoos*), disease (e.g., *get cancer*, *carry malaria*), reproductive behavior (e.g., *have a menstrual cycle*), and other miscellaneous behaviors (e.g., *pound their chests to display dominance*, *perform in the circus*); online sources about strange animal behaviors were consulted in order to find the more obscure properties.  -->

In addition, participants saw prevalence levels ranging from 10%, 30%, 50%, 70%, and 90%, as in @Cimpian2010 and Expt. 1 above.
In order to avoid fatigue and/or habituation to the statistics of the task, unlike Expt. 1, the exact prevalence seen on an individual trial was sampled from a (discrete) uniform distribution centered at the prevalence level (10, 30, ..., 90) with a range of $\pm 5\%$ (e.g., participants might see prevalence levels of 12%, 28%, 54%, 71%, 87%, etc...)

### Procedure

Participants were told that scientists had recently discovered lots of new animals that we did not know existed.
On each trial, they would learn a fact about a new animal and be asked whether or not they agreed with a sentence.

On each trial, participants read: "Scientists discovered an animal called a *K*. Out of all of the *Ks* on the planet, *X%* of them *F*" (e.g., "Scientists discovered an animal called a jav. Out of all of the javs on the planet, 9% of them have an exquisite sense of smell").
They were then asked if the corresponding bare plural sentence "*Ks F*"  was true or false (e.g., "Javs have an exquisite sense of smell").
Participants responded using radio buttons labeled "True" and "False", the ordering of which was randomized between-subjects.
As in Ch. 2, Expt. 2B, novel animal category names were mostly taken from @Cimpian2010 and similar studies on generic language. 
Each participant completed 25 trials, 5 at each prevalence level (10, 30, 50, 70, 90).
Each trial asked about a different property.

After the generic interpretation trials, participants completed a memory check trial wherein they were shown ten properties and asked to click on those they had seen in the experiment.
The list was comprised of 5 properties they had seen in the experiment and 5 properties they had not seen; the unseen properties were semantically similar to a seen property (e.g., *have brown fur* [seen] vs. *have long legs* [unseen]; *are afraid of loud noises* [seen] vs. *are afraid of dogs* [unseen]).
Following the memory check trials and depending on their ratings in the task, participants completed up to four explanation trials. 
On an explanation trial, participants saw a rating they had given for a property they had either rated as (1) true at prevalence levels 10% or 30% or (2) false at prevalence levels 70% or 90%, and asked if they could explain why they gave the response that they gave. 
(These data were used in an exploratory analysis.)
If participants gave no such ratings, they did not complete any explanation trials. 
The experiment can be viewed at http://stanford.edu/~mtessler/generics-asymmetry/experiments/endorsement-1.html.

### Results

```{r asymmetry model results}
load("cached_results/expt2_splithalf.RData") #df.end.1.atc.splithalf.r.summary
load("cached_results/expt2_aep_need_splithalf.RData") #df.end.aep_need.splithalf.r
load("cached_results/expt2_apriori_fit_summary.RData") # md.end1.atc.summary.apriori, md.end1.atc.summary
model.cortest <- with(md.end1.atc.summary %>% filter(optimality == 5, semantics == "uncertain"), cor.test(MAP, mean_atc))
```

97 participants were excluded for failing to pass the memory check using the preregistered exlcusion criteria (at least 4 out of 5 hits and at least 4 out of 5 correct rejections) and 3 participants were additionally excluded for self-reporting a native language other than English, leaving n=150 for these analyses.^[
  Because the exclusion criteria seem to be ill-calibrated to the difficulty of the memory check trials, we will also complete the analyses using participants who got 3/5 hits and 3/5 correct rejections, of which there are n=197 excluding the non-native English speakers.
]

We first compare the prevalence implied by a novel generic sentence (measured in Ch. 2, Expt. 2) to the average prevalence at which a participant endorses the generic, using the uniform prior analysis assumed by @Cimpian2010. 
We have already seen that our computational model predicts well the variability in implied prevalence (Ch. 2, Expt. 2). 
We thus use our computational model previously fit to the implied prevalence data to generate predictions for the average endorsement prevalence. 
The computational model predictions are dependent on a softmax parameter.
We report *a priori* prediction quantities setting this parameter equal to 5, and note that the rank ordering of these predictions (as opposed to the exact quantitative values) is relatively stable across different parameter settings. 
These exact quantitative predictions were preregistered.

Because of the mathematical properties of @Cimpian2010's *average endorsed prevalence*, the expected variability across items is very small: All model predictions range between 0.52 and 0.73 (total range = 0.21). 
Note that the same items have average implied prevalence ratings that range between 0.37 and 0.95 (total range 0.58).
Therefore, it is likely that this averaged endorsed prevalence is a much less reliable measurement than implied prevalence.
To estimate the theoretical maximum explainable variance $\rho$ in the data set, we compute split-half correlations by repeating splitting the data set in half by participants, computing averaged endorsed prevalence for each item in each split-half, computing correlation between two halves of the data set, and using the Spearman-Brown prediction formula to estimate predicted reliability with full sample size. 
This estimate of theoretical maximum correlation is itself highly variable ($\rho =$ `r format(df.end.1.atc.splithalf.r.summary[[1, "mean_r"]], digits = 2)`[`r format(df.end.1.atc.splithalf.r.summary[[1, "lower_r"]], digits = 2)`, `r format(df.end.1.atc.splithalf.r.summary[[1, "upper_r"]], digits = 2)`]), indicating variability in this derived measured is not highly reliable.

Our model's *a priori* model predictions for the average endorsed prevalence on a by-item basis showed a correlation with the data comparable to that of the explainable variance $r(`r model.cortest[["parameter"]][["df"]]`)$ = `r round(model.cortest[["estimate"]][["cor"]],2)`, $p = `r round(model.cortest[["p.value"]], 4)`$ (Figure\ \@ref(fig:aepCimpianScatter)).
The results were extremely comparable using for the more relaxed exclusion criteria, as well.
Thus the average endorsed prevalence measure does not exhibit much variance (both in the model and in the data) and the variance it does exhibit is not highly reliable. 

```{r aepCimpianScatter, fig.width=8, fig.asp=0.5, fig.cap="Average endorsed prevalence computed using a uniform need prior (Cimpian et al., 2010 method) in comparison to the generics endorsement model predictions."}

ggplot(md.end1.atc.summary, 
       aes(x = 100*MAP, xmin = 100* cred_lower, xmax = 100*cred_upper,
                                y = mean_atc, ymin = lower_atc, ymax = upper_atc))+
  geom_point(size = 2)+
  geom_errorbar(alpha =0.15)+
  geom_errorbarh(alpha = 0.15)+
  xlab("Model prediction")+
  ylab("Average endorsed prevalence (Cimpian method)")+
  coord_fixed()+
  scale_y_continuous(limits = c(45, 79))+
  scale_x_continuous(limits = c(50, 79))

```




We next examine the endorsement data by computing average endorsed prevalence assuming a need probability distribution that follows the environmental statistics of the property, operationlized using the prevalence prior elicited for these same items in Ch. 2 (Expt. 2a). 
Rather than take the linear average of the prevalence levels at which participants endorse the generic sentence, we weight these endorsements by the probability that the prevalence level would occur for that property.
For example, the property *is afraid of loud noises* has a 0.43 probability of occuring around 90% prevalence within a kind (averaging between 80-100% prevalence) whereas the property *carries malaria* only has a 0.09 probability of occuring with that same prevalence; the endorsement ratings are thus weighted to reflect these environmental statistics, providing a more accurate measure of the average prevalence at which we would expect speakers to use the generics.
Using this environmental need probability, the average endorsed prevalence shows a remarkably improved reliability ($\rho =$ `r format(df.end.aep_need.splithalf.r[[1, "mean_r"]], digits = 2)`[`r format(df.end.aep_need.splithalf.r[[1, "lower_r"]], digits = 2)`, `r format(df.end.aep_need.splithalf.r[[1, "upper_r"]], digits = 2)`]).

We perform the same transformations to the model predicted endorsements (again, generated *a priori* after fitting to the implied prevalence data). 





```{r aepScatter, fig.width = 10, fig.asp = 0.5, fig.cap="Here is a caption."}
load(file = "cached_results/expt_aep_comparisons.RData")
#df.end.1.aep.aip, m.samp.aep, df.prior.3.filtered.needProbs, df.end.1.aep

left_join(
  df.end.1.aep.aip %>%
    filter(src == "aep"), m.samp.aep %>% filter(optimality == 5)
) %>% 
  summarize( r = cor(aep_mean, MAP))
fig.aep_need.modelscatter <- left_join(
  df.end.1.aep.aip %>%
    filter(src == "aep"), m.samp.aep %>% filter(optimality == 5)
) %>%
  ggplot(., aes( y = aep_mean, ymin = aep_lower, ymax = aep_upper,
                 x = MAP, xmin = cred_lower, xmax = cred_upper))+
  geom_errorbar(alpha = 0.3)+
  geom_errorbarh(alpha = 0.3)+
  geom_point()+
  #coord_fixed(ratio = 100)+
  geom_abline(intercept = 0, slope = 1, lty = 3, alpha = 0.3)+
  xlim(27, 82)+
  ylim(27, 82)+
  coord_fixed()+
  xlab("Model prediction")+
  ylab("Average endorsed prevalence\n(need probability)")


# prior expectation only (i.e., uniform endorsement model)
fig.aep_need.priorscatter <- left_join(
  df.end.1.aep, df.prior.3.filtered.needProbs %>%
  group_by(property) %>%
  summarize(uniformLikelihood_aep = sum(prevalence_level*normalized_need_prob))
) %>%
  ggplot(., aes( y = need_endorsed_prevalence,
                 x = uniformLikelihood_aep))+
  #geom_errorbar(alpha = 0.3)+
  #geom_errorbarh(alpha = 0.3)+
  geom_point()+
  #coord_fixed(ratio = 100)+
  geom_abline(intercept = 0, slope = 1, lty = 3, alpha = 0.3)+
    xlim(27, 82)+
  ylim(27, 82)+
  coord_fixed()+
  xlab("Model prediction\n(Uniform endorsement)")+
  ylab("Average endorsed prevalence (need probability)")


# prevalence endorsement model
fig.aep_need.invariantscatter <- left_join(
  df.end.1.aep, df.prior.3.filtered.needProbs %>%
  mutate(prev_posterior  = prevalence_level * normalized_need_prob) %>%
  group_by(property) %>%
  mutate(normalized_prev_posterior  = prev_posterior /sum(prev_posterior)) %>%
  summarize(prevLikelihood_aep = sum(prevalence_level*normalized_prev_posterior))
) %>%
  ggplot(., aes( y = need_endorsed_prevalence,
                 x = prevLikelihood_aep))+
  #geom_errorbar(alpha = 0.3)+
  #geom_errorbarh(alpha = 0.3)+
  geom_point()+
  #coord_fixed(ratio = 100)+
  geom_abline(intercept = 0, slope = 1, lty = 3, alpha = 0.3)+
    xlim(27, 82)+
  ylim(27, 82)+
  coord_fixed()+
  xlab("Model prediction\n(Context invariant endorsement)")+
  ylab("Average endorsed prevalence (need probability)")


cowplot::plot_grid(
  fig.aep_need.modelscatter,
  fig.aep_need.priorscatter + theme(axis.title.y = element_blank()),
  fig.aep_need.invariantscatter + theme(axis.title.y = element_blank()),
  nrow = 1,
  rel_widths = c(1, 0.9, 0.9),
  labels = c("A", "B", "C")
)
```


```{r aepAipScatter, fig.width = 7, fig.asp = 0.5, fig.cap="Here is a caption."}
df.end.1.aep.aip %>%
  mutate(src = factor(src, levels = c("aep_unif", "aep"),
                      labels = c("Uniform need probability (Cimpian et al., 2010)",
                                 "Environmental need probability"))) %>%
    ggplot(., aes( y = mean, ymin = ci_lower, ymax = ci_upper,
                 x = aep_mean/ 100,
                 xmin = aep_lower / 100,
                 xmax = aep_upper / 100))+
  geom_errorbar(alpha = 0.3)+
  geom_errorbarh(alpha = 0.3)+
  geom_point()+
  facet_wrap(~src)+
  #coord_fixed(ratio = 100)+
  geom_abline(intercept = 0, slope = 1, lty = 3, alpha = 0.3)+
  xlab("Average endorsed prevalence")+
  ylab("Average implied prevalence")+
  xlim(0.25, 1)+
  ylim(0.25, 1)
```


# Discussion


# References

<!-- ```{r create_r-references} -->
<!--  r_refs(file = "generics.bib") -->
<!-- ``` -->

<!-- \setlength{\parindent}{-0.5in} -->
<!-- \setlength{\leftskip}{0.5in} -->

<div id = 'refs'></div>

\newpage
# Appendix 

The following shows the symmetry between average implied prevalence and average endorsed prevalence when a uniform prior on prevalence is used. 

Let us first assume that the prevalence prior is uniform; that is, $P(r) \propto 1$, .
That makes, $L(r \mid u) \propto r$.
To compute the expectation of $L(r \mid u)$, we must first normalize the distribution:

\begin{eqnarray}
L(r \mid u) = \frac{r}{Z}, \text{where } Z = \int_0^1r\cdot \diff r \\
 \int_0^1r\cdot \diff r = \frac{1}{2}r^2 \Big|_0^1  = \frac{1}{2}
\end{eqnarray}

therefore:  $L(r \mid u) = 2r$. 
The average implied prevalence is then:

\begin{eqnarray}
\mathbb{E}_{r \sim L(r \mid u)}[r] = \int_0^1r \times 2r \diff r \\
= \int_0^12r^2 \diff r = \frac{2}{3} r^3 \Big|_0^1 = \frac{2}{3}
\end{eqnarray}

The average implied prevalence given a generic statement assuming a uniform prior over prevalence is 66%.
What is the average endorsed prevalence?
<!-- Again, assuming a uniform prior: -->
\begin{align}
S(u \mid r) \propto L(r \mid u) = (2r)^\alpha
\end{align}
Assume $\alpha = 1$. 
Then $S(u \mid r) = 2r$. 
For this example, the need distribution is the same as the uniform distribution assumed by @Cimpian2010.
Computing average endorsed prevalence via Bayes' rule then becomes: 
$$S_{endorsed}(r \mid u) \propto 2r \times P(r) = 2r \times 1 = 2r$$
the expectation of which will also be $\frac{2}{3}$.
Thus, there is also a theoretical symmetry between average endorsed and implied prevalence for a generic when assuming uniform priors on prevalence.\footnote{
In this case, increasing $\alpha$ produces an asymmetry in the unpredicted direction. For example, if $\alpha = 2$: 

\begin{eqnarray}
S(u \mid r) \propto (2r)^2 = 4r^2
\end{eqnarray}

after normalization, this becomes: $S(u \mid r) = 3r^2$. Then, to calculate 

\begin{eqnarray}
\mathbb{E}_{r \sim S(r \mid u)}[r] = \int_0^1r \times 3r^2 \diff r \\
= \int_0^1 3r^3 \diff r = \frac{3}{4} r^4 \Big|_0^1 = \frac{3}{4}
\end{eqnarray}
}



