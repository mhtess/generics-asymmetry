---
title             : "The asymmetry between generic truth conditions and implied prevalence"
shorttitle        : "Generic asymmetry"

author: 
  - name          : "Michael Henry Tessler"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Serra Mall, Bldg. 420, Rm. 316, Stanford, CA 94305"
    email         : "mhtessler@stanford.edu"
  - name          : "Noah D. Goodman"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"
    
header-includes:
  - \usepackage{tabularx}
  - \usepackage{multicol}
  - \usepackage{wrapfig}
  - \usepackage{caption}
  - \usepackage{booktabs}
  - \usepackage{amsmath}
  - \usepackage{graphicx}
  - \usepackage{subcaption}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  
author_note: >
  This manuscript is currently in prep. Comments or suggestions should be directed to MH Tessler.

abstract: |
  Enter abstract here. Each new line herein must be indented, like this line.
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["generics.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no

class             : "man"
output            : papaja::apa6_pdf
---
\newcommand{\denote}[1]{\mbox{ $[\![ #1 ]\!]$}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\definecolor{Red}{RGB}{255,0,0}
\definecolor{Green}{RGB}{10,200,100}
\definecolor{Blue}{RGB}{10,100,200}

\newcommand{\mht}[1]{{\textcolor{Blue}{[mht: #1]}}}
\newcommand{\ndg}[1]{{\textcolor{Green}{[ndg: #1]}}}
\newcommand{\red}[1]{{\textcolor{Red}{#1}}}


```{r load_packages, include = FALSE}
library("papaja")
library(tidyverse)
library(cowplot)
library(ggthemes)
library(RColorBrewer)
library(ggpirate)
theme_set(theme_few())
```

```{r analysis_preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, cache=T, message=FALSE, sanitize = T)

```

# Introduction

\red{look at things that cite Cimpian et al. (2010) and see if people are discussing the asymmetry in the philosophical literature}

<!-- 
- Note about large-scale truth judgment task: 
  - Cimpian fudges the analysis a little bit: He does it on a subject-wise basis, but using different properties (participants dont rate purple feathers for 10%, 30%, ...)... if we want to do item-wise, we either need to give subjects all the prevalence levels, or do some fancy hierarchical bayesian modeling (the latter may be preferred)
-->


<!-- Generic statements (e.g., "Birds fly") convey generalizations about categories [@Carlson1977; @Carlson1995; @Leslie2008]. -->
<!-- This kind of language has caught the attention of psychologists, linguists, and philosophers beacuse it is widely prevalent in everyday conversation and child-directed speech [@Gelman1998; @Gelman2008; @GelmanEtAl2004], while at the same time having formal properties that are difficult to characterize [@Carlson1995].  -->

In the last chapter, we saw how generic statements can be intuitively true under a wide range of statistical conditions. 
"Birds fly" is true of most birds, but "Ducks lay eggs" is only true of adult, fertile, female ducks, which make up less than 50\% of the category.
"Mosquitos carry malaria" is intuitively true despite less than 1\% of category actually having the property.
Despite these variable truth conditions, it is thought that generic statements often carry the *implied prevalence* of a universal or near-universal quantifier [@Abelson1966; @Gelman2002; @Cimpian2010]. 
For example, when both children and adults learn "Bears like to eat ants", they tend to think that *almost all* bears like to eat ants [@Gelman2002].

There is a surprising d\'{e}colage between truth judgments and interpretations of generic sentences: Interpretations are often strong while truth conditions are flexible. 
@Cimpian2010 found that upon reading a novel generic sentence (e.g., "Glippets have yellow fur."), participants infer that the implied referent prevalence is high (e.g., almost all glippets have yellow fur).
At the same time, when participants are confronted with evaluating the same statement based on weak statistical evidence (e.g., 30% of glippets have yellow fur), participants are still willing to endorse the statement.
This results in an asymmetry: Generics are accepted for a wide range of prevalence levels but are interpreted as implying high prevalence.

Of course, generics need not imply high prevalence.
We saw in Chapter 2 that statements like "Feps live in zoos" are interpreted as applying to relatively few feps, while "Feps eats insects" is taken to apply *almost all* feps. 
The sensitivity of implied prevalence, we showed, was a result of diverse prior knowledge about properties interacting with the uncertain semantics of generics. 
Accordingly, @Cimpian2010 (Expt. 3) found a significant reduction in the asymmetry for generics about accidental properties (e.g., "Glippets have wet fur."), mostly driven by those statements carrying substantially weaker interpretations. 
Endorsements for generics are similarly sensitive to the property (e.g., "Birds lay eggs" vs. "Birds are female"), but the results of @Cimpian2010 suggest the sensitivity of endorsement is less than that of interpretations.

When and how does the asymmetry between truth conditions and implied prevalence manifest?
<!-- \red{[check cimpian's discussion section]} PERHAPS ADDRESS LATER WHAT CIMPIAN SAYS, because it's not directly the point (his ideas are tooo vague) -->
Here, we draw on the same computational framework to address this question. 
First, we describe how endorsements and implied prevalence are compared and show through simulation various conditions under which the asymmetry appears and when it is predicted to disappear or even reverse.
Then, we replicate the original asymmetry findings of @Cimpian2010 while expanding the stimulus set to reveal even more variability in the asymmetry.
We then extend these findings to a larger-scale study using a more diverse stimulus set.
In addition to revealing the quantitative intracacies of the asymmetry, our very same model is able to account for both truth conditions and implied prevalence simultaneously. 


<!-- Upon hearing "Feps eat insects", listeners come to think *almost all* feps eat insects; the possibility that only 10% of them eat insects does not immediately come to mind.  -->
<!-- However, if we *knew* that 10% of feps ate insects, the generic "Feps eat insects" does not seem at all false.  -->

<!-- By contrast, participants endorse generics for a wide range of prevalence levels (e.g., even when 30\% of glippets have yellow fur).  -->
<!-- By quantitatively comparing the two measurements, @Cimpian2010 found an asymmetry between between truth conditions and implied prevalence of generic statements but not for quantified statementse involving *all* or *most*. -->

<!-- Not all generics have strong interpretations, however.  -->
<!-- "Mosquitos carry malaria" really seems to only have the quantificational force of an existential claim, meaning *some \mosquitos carry malaria*. -->
<!-- Weak interpretations from generics have been observed in tasks having participants draw inferences about individual members of a kind [@Khemlani2009; @Khemlani2012] as well as asking about implied prevalence [@Cimpian2010; Expt. 3]. -->
<!-- Coupled with the truth conditions measurements, @Cimpian2010 found that a significant reduction in the asymmetry for generics about accidental properties (e.g., "Glippets have wet fur."). -->



# The asymmetry between endorsements and interpretations

The asymmetry between endorsements and interpretations results from comparing participants' responses on  a two-alternative forced choice (2AFC) truth judgment task (as explored in Chapter 3) to implied prevalence ratings, typically on a 101-pt scale (e.g., the percentage of the category with the property; Chapter 2).
These two measurements are not directly comparable because of the different scales.

In order to compare the two, @Cimpian2010 took the 2AFC endorsement data and computed the average prevalence level at which individual participants endorsed the generic (*average endorsed prevalence*).
A single participant's average endorsed prevalence is the mean of the prevalence levels for which the participant endorsed the generic. 
For example, if a participant endorsed the generic at 50%, 70%, and 90% prevalence levels, their *average endorsed prevalence* would be 70%. 
As a participant becomes more lenient with the generic (endorsing it at lower prevalence levels), their average endorsed prevalence decreases; as a participant becomes more conservative (e.g., only endorsing the generic when the property applies to *all* or *almost all*), their average endorsed prevalence increases. 
The theoretical minimum average endorsed prevalence is 50%, for a participant who endorsed the generic at all prevalence levels.^[
  Technically, average endorsed prevalence could be less than 50% were a participant to endorse the statement at low prevalence levels and then reject the statement at high prevalence levels. 
]
If a participant fails to endorse the generic for all prevalence levels, @Cimpian2010 assigned them an average endorsed prevalence of 100%, because presumably the participant would only endorse the generic if it were true of 100% of members.
<!-- @Cimpian2010's analysis transforms the endorsement data onto the prevalence scale and behaves intuitively to how one would think the construct should behave. -->
<!-- As a participant becomes more lenient with the generic (endorsing it at lower prevalence levels), their average endorsed prevalence decreases; as a participant becomes more conservative (e.g., only endorsing the generic when the property applies to *all* or *almost all*), their average endorsed prevalence increases.  -->

With the exception of how the analysis handles participants who never endorse the statement, @Cimpian2010's analysis is a participant-wise application of Bayes' rule.
Each task each measures a conditional probability. 
The implied prevalence task measures the probability of different prevalence levels $p$ given a generic statement $u$: $P_{implied}(p \mid u)$.
The endorsement tasks measure the probability of endorsing a generic utterance $u$ given different prevalence levels: $P_{endorsed}(u \mid p)$. 
These two conditional probabilities are related via Bayes' rule: $P(p \mid u) \propto P(u \mid p) \cdot P(p)$.
For the asymmetry analysis, the mean implied prevalence $\sum_{p} p \cdot P_{implied}(p \mid u)$ is compared to the expected endorsement prevalence $\sum_{p} p\cdot P_{endorsed}(p \mid u)$, where $P_{endorsed}(p \mid u) \propto P_{endorsed}(u \mid p) \cdot P(p)$.
@Cimpian2010's analysis applies this calculation assuming a uniform prior on prevalences $P(p)$.
Recognizing the formal relationship behind @Cimpian2010's calculation makes explicit its assumptions (e.g., the uniform prior on $p$).
Further, seeing the mathematical relationship provides a way to apply the same analysis on a by-item, as opposed to a by-participant, basis.

Because of the uniform prior, computing $P_{endorsed}(p \mid u)$ amounts to take the proportion of participants who endorse the generic at each prevalence level and normalizing those proportions to create a probability distribution. 
For example, if the proportions of endorsements for the five prevalence levels (10, 30, 50, 70, 90%) is $[0.2, 0.2, 0.6, 0.9, 0.9]$, the probability distribution $P_{endorsed}(p \mid u)$ would be [0.07, 0.07, 0.21, 0.32, 0.32]. 
This distribution is arrived at by taking the proportion of endorsements and dividing it by the sum of all endorsement proportions (e.g., $0.07 = \frac{0.2}{0.2 + 0.2 + 0.6 + 0.9 + 0.9}$, $0.32 = \frac{0.9}{0.2 + 0.2 + 0.6 + 0.9 + 0.9}$).
We then treat this distribution as a distribution over prevalence and take the expectation (i.e., the mean) of that distribution: $0.07 \times 10\% + 0.07 \times 30\% + 0.21 \times 50\% + 0.32 \times 70\% + 0.32 \times 90\% = 65\%$.

## Computational model predictions

From Chapters 2 and 3, we know that the following are good models for $P_{implied}(p \mid u)$ and $P_{endorsed}(u \mid p)$:

\begin{eqnarray}
L(p \mid u) \propto p \cdot P(p) \\
S(u \mid p) \propto L(p \mid u) ^ \alpha
\end{eqnarray}

Let us assume that $P(p)$ is uniform; that is, $P(p) \propto 1$.
That makes, $L(p \mid u) \propto p$.
To compute the expectation of $L(p \mid u)$, we must first renormalize the distribution:

\begin{eqnarray}
L(p \mid u) = \frac{p}{Z}, \text{where } Z = \int_0^1p\cdot \diff p \\
 \int_0^1p\cdot \diff p = \frac{1}{2}p^2 \Big|_0^1  = \frac{1}{2} \\
\end{eqnarray}

therefore:  $L(p \mid u) = 2p$.

\begin{eqnarray}
\mathbb{E}_{p \sim L(p \mid u)}[p] = \int_0^1p \times 2p \diff p \\
= \int_0^12p^2 \diff p = \frac{2}{3} p^3 \Big|_0^1 = \frac{2}{3}
\end{eqnarray}

Again, assuming a uniform prior:
\begin{align}
S(u \mid p) \propto L(p \mid u) ^ \alpha = (2p)^\alpha
\end{align}
If $\alpha = 1$, then we just have $S(u \mid p) = 2p$. Then, computing average endorsed prevalence via Bayes' rule, we get $S_{endorsed}(p \mid u) \propto 2p \times 1 = 2p$, the expectation of which will also be $\frac{2}{3}$.

What happens if $\alpha = 2$? 

\begin{eqnarray}
S(u \mid p) \propto (2p)^2 = 4p^2
\end{eqnarray}

after normalization, this becomes: $S(u \mid p) = 3p^2$. Then, to calculate 

\begin{eqnarray}
\mathbb{E}_{p \sim S(p \mid u)}[p] = \int_0^1p \times 3p^2 \diff p \\
= \int_0^1 3p^3 \diff p = \frac{3}{4} p^4 \Big|_0^1 = \frac{3}{4}
\end{eqnarray}

<!-- However, because participants in @Cimpian2010's experiment rate generics about each property only once (with only one prevalence level; e.g., 30% have yellow fur), this analysis collapses across generics of individual items used in the experiment. -->
<!-- Unless a participant provides judgments at all prevalence levels (10, 30, 50, 70, 90%) for a particular item (e.g., have yellow fur), then the average endorsed prevalence for a particular item cannot be computed.  -->
<!-- Thus, we propose a similar, alternative formulation of averaged endorsed prevalence derived from basic probability theory.  -->
<!-- Our formulation will generalize to item-wise analysis we explore in the empirical part of this paper. -->


<!-- An asymmetry between truth conditions and interpretations occurs when the prevalence levels at which a statement is endorsed differ from the prevalences implied by the same statement.  -->
<!-- The asymmetry is present generic statements about biological properties, as reported by @Cimpian2010. -->
<!-- Generic statements are accepted for a wide range of prevalence levels (e.g., even when the property is present in only 10% of cases) but when participants learn from generics, the statements are often interpreted quite strongly (e.g., all or almost all have the property).  -->

<!-- ## Technical description -->

<!-- The empirical asymmetry between truth conditions and interpretations was reported by @Cimpian2010. -->
<!-- Truth conditions data result from an endorsement task, a two-alternative forced choice (2AFC) paradigm (as explored in Chapter 3). -->
<!-- Interpretation data are prevalence ratings, typically on a 101-pt scale (e.g., the percentage of the category with the property). -->
<!-- Thus, these two constructs are not in the same measurement space; one must be transformed into the other in order to compare them. -->


This asymmetry is anomalous (at least in theory) when compared again quantifiers.
*All* is the easiest to think about: On a true/false task, participants should only endorse an *all* quantified statement when the prevalence is 100%; separately, when participants learn from a sentence containing *all*, they should believe the prevalence is 100%. 
From a simplified analysis, the quantifier *most* is true when the prevalence is greater than 50% (i.e., *most* means roughly *more than half*); the average prevalence at which a person will assent to a statement involving *most* will be 75% (i.e., all prevalences between 50-100%).
Given a uniform prior over the prevalence, a literal listener will interpret *most* as being consistent will all prevalences greater than 50%; again, averaging over these possilibities, the mean implied prevalence will be 75%.^[
  The lack of the asymmetry for *most* was shown in @Cimpian2010 Expt. 1.
]
On a literal analysis, *some* behaves similarly: It rules out prevalence of 0%, and leaves open all other possibilities; on a literal analysis, the average truth conditions and the mean implied prevalence should be about 50%.

## Criticisms of the asymmetry analysis

Translating this theoretical analysis into predictions about observed behavior is less than straightforward.^[
  It is not my goal to delve too deep into the intracacies of comparing truth judgments with prevalence ratings, this being part of more broadly a comparison between speaker and listener behavior. 
  I will not present models of other quantifiers and show quantifiers-by-domain interactions, though this is clearly an area ripe for modeling. 
  Instead, I will try to highlight some of the issues, as I see it, that will help us better understand the comparison of truth judgments and implied prevalence ratings for generic statements, specifically. 
]
I believe literal endorsement tasks (as the *truth conditions* task purports to be) are possible. 
In @Yoon2016 and @Yoon2017, we attempted to empirically measure "literal semantics" by providing participants with both a state of the world and an utterance and asking the participant if they thought that the speaker thought the utterance was true (e.g., "John baked a cake. Sally thought the cake deserved a rating of 5 out of 5. Do you think Sally thought John's cake was good?").
Here we found no effects of pragmatic production behavior (e.g., participants rated the adjective "good" as equally good for 4 out of 5 and for 5 out of 5, even the adjective "amazing" was also present on different trials in the experimental context).
The setup of very similar to the truth conditions task of @Cimpian2010 wherein participants are supplied with both a state of the world (here, prevalence) and an utterance (a generic) and asked if they thought it was true or false.
However, @Cimpian2010 Expt. 2 show results for the truth conditions task using the quantifier "some" and find less than full endorsement for "some" at 70% and 90% prevalence. 

Attempting a literal interpretation task is even trickier. 
Language understanding is in general a pragmatic process which should take into account both a listener's prior beliefs and their assumptions of the speaker.
Therefore, in an implied prevalence task involving *some* (e.g., "Some lorches have purple feathers"), we would expect participants to report a prevalence consisntent with *some but not all* (perhaps even *some and not most*). Additionally, prior beliefs can rarely be assumed to be uniform. 
In our empirical work thus far, we have seen how prevalence prior distributions vary dramatically across different kinds of properties and we already know (from Chapter 2) that these influence participants' implied prevalence ratings.


<!-- The above analysis assumes uniform priors over prevalence and a literal interpretation model (on the implied prevalence side) as well as a model of a literal speaker (on the truth conditions side).  -->
<!-- Were any of these assumptions to prove false, the theoretical symmetries would break.  -->
<!-- @Cimpian's data is from language experiments, for which pragmatic understanding should be the default assumption.  -->
<!-- It is possible the truth conditions task should also be thought of as pragmatic. -->
<!-- @Degen's gumballs show that participants are reluctant to endorse a *some* statement when *all* is present. -->
<!-- For example, the implied prevalence task can easily be construed as a pragmatic  -->


<!-- 
Technical aspects of the asymmetry:
  - Theoretical exploration using "some", "most" as simply threshold-functions (no speaker) and Uniform priors for the interpretation model
    - This will show the symmetry for both?
    - What will it show for the generic? (what about an L1?)
  - For some, most: interpretation is potentially more complicated (some has the not all [not most?] implicature, which Cimpian finds... "most" maybe as well?)
  - The dynamic range for the asymmetry is more limited for ATC than IP, so must of the action happens because of IP
  
  - Cimpian did it subject-wise but not property-wise (assuming property classes and equivalence within a class...). How do you do it propery-wise?
    - [ ] do you get similar results by bootstrapping?
    - [ ] can you build a bayesian model of these data?
    - one possibility is to have subjects rate all prevalence levels... but this may be strange for people
    - another is to share statistical strength
      - assume there is a threshold for each property
      - each participant's threshold is a sample from this distribution...
      - but properties are completely independent? still may be too sparse
        - maybe you don't need the participant level: just have a threshold model that has the possibility of noise?
-->

\red{Figure 1} shows the average truth conditions for a variety of semantic hypotheses.

```{r asymmetrySimulations, fig.width=8, fig.asp=0.5, cache=F, fig.cap="Model simulations"}

load(file = "cached_results/modelSims-priors_fixedT_uncertainT.RData")
get.colors <- function(pal) brewer.pal(brewer.pal.info[pal, "maxcolors"], pal)
spectrum.color.palette <- get.colors("Blues")
load("cached_results/modelSims-asymmetry.RData")

fig.sims.priors <- ggplot(sims.combined %>%
         filter(
           PriorShape %in% c("uniform", "biological_rare", "accidental_rare"),
           src %in% c("priors")
           ) %>%
         mutate(PriorShape = factor(PriorShape, levels = c("uniform", "biological_rare", "accidental_rare"),
                                    labels = c("uniform", "biological", "accidental")),
                src = factor(src, levels = c( "priors"),
                             labels = c(
                               '\n prevalence prior'
                                        ))), 
       aes(x = state))+
    geom_density(aes(y = ..scaled..), fill= 'black', size = 0.6, alpha = 0.7, adjust = 1.1)+
    theme_few() +
    scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    ylab("Probability density (scaled)") +
    xlab("Prior Prevalence")+
    #scale_color_solarized()+
    #scale_fill_solarized()+
    facet_grid(PriorShape~src, scales = 'free')+
    theme(strip.text.y = element_text(angle = 0),
          legend.position = "none"
          )


fig.sims.distributions <- ggplot(sims.combined %>%
         filter(
           PriorShape %in% c("uniform", "biological_rare", "accidental_rare"),
           src == "posteriors"
           #src %in% c("fixed_0.1", "fixed_0.33", "fixed_0.5", "posteriors")
           ) %>%
         mutate(PriorShape = factor(PriorShape, levels = c("uniform", "biological_rare", "accidental_rare"),
                                    labels = c("Xs Y\n[uniform]", "Xs fly\n[biological]", "Xs carry malaria \n [accidental]")),
                src = factor(src, levels = c("posteriors"),
                             labels = c('generic\n(uncertain threshold)'))),
                # src = factor(src, levels = c( "fixed_0.1", "fixed_0.33",
                #                               "fixed_0.5", "posteriors"),
                #              labels = c(
                #            #    'prevalence prior',
                #                '"some"\n(threshold = 0.01)',
                #                '"more than a third"\n(threshold = 0.33)',
                #               '"most"\n(threshold = 0.5)',
                #                'generic\n(uncertain threshold)'
                #                         ))), 
       aes(x = state, fill = src, color = src))+
    geom_density(aes(y = ..scaled..), 
                 color ='black', fill = 'grey50', size = 0.6, alpha = 0.7, adjust = 1.1)+
    theme_few() +
    scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    ylab("Scaled posterior density") +
    xlab("Implied Prevalence")+
    # scale_fill_brewer(palette=2, type = "seq", direction = -1)+
    # scale_color_brewer(palette=2, type = "seq", direction = -1)+
    #scale_color_manual( values =  c(spectrum.color.palette[c(8,5,2)], "#238b45"))+
    #scale_fill_manual( values = c(spectrum.color.palette[c(8,5,2)], "#238b45") )+
    facet_grid(PriorShape~src, scales = 'free')+
    theme(strip.text.y = element_blank(),# element_text(angle = 0, size = 12),
          legend.position = "none",
          axis.title.y = element_blank())

fig.sims.truthJudgments <- s1.simulations %>%
  filter(model == "generic") %>%
  mutate(prior = factor(prior, levels = c("uniform", "biological_rare", "accidental_rare"),
                                    labels = c("uniform", "biological", "accidental"))) %>%
  ggplot(., aes ( x = referent_prevalence, y = endorsement,
                  linetype = prior))+
  geom_point()+
  geom_line()+
  theme(legend.position = c(0.8, 0.4) ,
        legend.title = element_blank())+
    scale_x_continuous(breaks = c(0, 0.5, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 0.5, 1), limits= c(0, 1))+
  ylab("Endorsement probability")+
  xlab("Referent prevalence")


fig.sims.asymmetry <- m.gen.asym %>%
  filter(src == "posteriors") %>%
  mutate(src = factor(src),
         PriorShape = factor(PriorShape, levels = c("biological_rare","accidental_rare",   "uniform"),
                                    labels = c( "biological",  "accidental","uniform")),
         model = factor(model, levels = c("S1", "L0"), labels = c("Truth conditions", "Implied prevalence"))) %>%
  ggplot(., aes(x = PriorShape, y = expval, 
                fill = model))+
  scale_fill_solarized() + 
      scale_y_continuous(breaks = c(0, 0.5, 1), limits= c(0, 1))+
  ylab("Average prevalence")+
  geom_col(position= position_dodge(), color = 'black')+
  #geom_errorbar(position = position_dodge())+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
        legend.position = c(0.80, 0.85),
        legend.title = element_blank(),
        axis.title.x = element_blank())


cowplot::plot_grid(
  fig.sims.priors + theme(plot.margin = unit(c(6, 3, 6, 0), "pt")), 
  fig.sims.distributions + theme(plot.margin = unit(c(6, 6, 6, 0), "pt")),
  cowplot::plot_grid(
      fig.sims.truthJudgments + theme(plot.margin = unit(c(18, 3, 6, 0), "pt")), 
      fig.sims.asymmetry + theme(plot.margin = unit(c(6, 0, 6, 0), "pt")),
      ncol = 1,
      labels = c("C","D"),
      rel_heights  = c(1, 1.25)
      ),
  #fig.sims.bars+ theme(plot.margin = unit(c(6, 0, 0, 0), "pt")),
  nrow = 1,
  labels = c("A", "B", ""),
  rel_widths = c(0.8, 0.6, 1)
)
```
<!-- - Model figure: -->
<!--   - Prior1, L("Generic1"), S: x=%, y= endorse, group = prior,  ATC & IP: bar plot (x = model) -->
<!--   - Prior2, L("Generic2"), S: x=%, y= endorse,  ATC & IP: bar plot (x = model) -->
<!--   - Prior3, L("Generic3"), S: x=%, y= endorse,  ATC & IP: bar plot (x = model) -->

<!-- 
Computing ATC for model should just be the mean of the production probabilities...
This probably is the same as the Cimpian sampling method, though won't have a way of constructing the variance..
thus the sampling method may be preferred for constructing the variance
-->

The computational model of generic interpretation predicts that the implied prevalence of a generic statement is an interaction between background knowledge (formalized as a prevalence prior) and an underspecified threshold semantics. 
I have shown in Chapter 2 how this model predicts context-sensitive interpretations of generic sentences. 
In Chapter 3, we saw how a speaker model can account for endorsement beahvior.
The implied prevalence of a generic may be very strong (e.g., "Wugs have four legs") or very weak (e.g., "Lorches live in zoos").
The implied prevalence of a generic 

# Experiment 1: Replication and extension of Cimpian et al. (2010) truth conditions task

In Chapter 2, we replicated @Cimpian2010's implied prevalence paradigm using a slightly expanded stimulus set (Expt. 1B). 
To replicate the asymmetry findings, we must also measure endorsements using @Cimpian2010's *truth conditions* paradigm.
Here, we replicate the truth conditions task using the slightly expanded stimulus set of @Cimpian2010.
The relevant prevalence priors have already been measured in Chapter 2 (Expt. 1A).

## Method

### Participants

We recruited 40 participants over MTurk.  
We chose a sample size at least twice as large as the original study by @Cimpian2010 (original $n = 20$), and to match that of Ch. 2 Expt. 1B. 
All participants were self-reported native English speakers and none completed the analagous implied prevalence task (Ch. 2 Expt. 1B).
The experiment took about 5 minutes and participants were compensated \$0.60.

### Procedure and materials

The cover story and materials were the same as in Ch. 2, Expt. 1B and the procedure followed that of @Cimpian2010 Expt. 1.
On each trial, participants were given a statement about a property's prevalence within a novel kind (*referent prevalence*; e.g. "50% of feps have yellow fur."). 
Participants were then asked whether or not they agreed or disagreed with the corresponding generic sentence (e.g. "Feps have yellow fur."). 
Referent prevalence varied between 10, 30, 50, 70, and 90%.

The experiment consisted of 25 trials: 5 trials for each of 5 types of properties measured in Ch. 2, Expt. 1A (part, color part, vague part, common accidental, rare accidental). 
Each prevalence level appeared once for each property type (5 prevalence levels x 5 property types). 

### Results

```{r figCbgResults, fig.width=10.5, fig.asp = 0.35, fig.cap="Words"}
load(file = "cached_results/cbg_results.RData")

dodge_width <- 7

fig.cbg.tj <- df.c.endorse_by_prev %>%
  mutate("Property type" = factor(stim_type,
                                         levels = c("part",
                                                    "vague",
                                                    "color",
                                                    "accidental"),
                                         labels = c("body part",
                                                    "color adj + part",
                                                    "gradable adj + part",
                                                    "accidental"))) %>%
  ggplot(., aes( x = stim_prevalence, y = mean, ymin = ci_lower,
                 ymax = ci_upper, fill = `Property type`,
                 color = `Property type`))+
  geom_line(position = position_dodge(dodge_width), alpha = 0.4,
            linetype = 3)+
  geom_linerange(position = position_dodge(dodge_width), alpha = 0.6,
                 size = 1)+
  geom_point(position = position_dodge(dodge_width),
             size = 2.5, shape = 21, color = 'black')+
  scale_fill_solarized()+
  scale_color_solarized()+
  ylab("Proportion endorse")+
  xlab("\n Referent prevalence")+
  scale_x_continuous(limits = c(0, 100), breaks = c(10,  30, 50, 70,90))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  theme(legend.position = c(0.75, 0.27))+
  coord_fixed(ratio = 65)

dodge_width <- 0.9

fig.cbg.asym <- bind_rows(
  df.c.int.bs %>% mutate(src = 'implied prevalence'),
  df.atc %>% mutate(src = 'truth conditions')
) %>%
  mutate("Property type" = factor(stim_type,
                                         levels = c("part",
                                                    "vague",
                                                    "color",
                                                    "accidental"),
                                         labels = c("body part",
                                                    "color adj + part",
                                                    "gradable adj + part",
                                                    "accidental")),
         src = factor(src, levels = c("truth conditions",
                                      "implied prevalence"))) %>%
  ggplot(., aes( x = `Property type`, y = mean, ymin = ci_lower,
                 ymax = ci_upper, fill = `Property type`, alpha = src))+
  geom_col(position = position_dodge(dodge_width), width = dodge_width,
           color = 'black')+
  geom_errorbar(position = position_dodge(dodge_width),
                 size = 1, width = 0.1)+
  scale_fill_solarized()+
  scale_alpha_manual(values = c(0.6, 1))+
  scale_y_continuous(limits = c(0, 100))+
  ylab("Average truth conditions")+
  guides(fill = F)+
  theme(axis.text.x = element_text(angle = 45, vjust =1 ,hjust = 1),
        legend.title = element_blank(),
        axis.title.x = element_blank()#,
        #legend.position = c(0.75, 0.9)
        )

cowplot::plot_grid(
  fig.cbg.tj + theme(plot.margin = unit(c(6, 3, 30, 0), "pt")), 
  fig.cbg.asym + theme(plot.margin = unit(c(6, 6, 6, 0), "pt")),
  #fig.sims.bars+ theme(plot.margin = unit(c(6, 0, 0, 0), "pt")),
  nrow = 1,
  labels = c("A", "B"),
  rel_widths = c(1,1)
)

```


For both behavioral data and model predictions (Eq. \ref{eq:S1}) we computed the average prevalence that led to an assenting judgment (the *average truth conditions*), for each property type and participant, following the procedure used by @Cimpian2010 and described in the previous section.

For our pair of models, there are two parameters (the two speaker optimality parameters).
We infer them using the same Bayesian data analytic approach as before. 
The MAP and 95\% HPD intervals for $\lambda_1$ is 19.5 [10.5, 19.9] and $\lambda_2$ is 0.4 [0.34, 0.49].
We then subjected the generic endorsement model to the same procedure as the human data. % subjected our model to the same procedure. 

The speaker model $S_2$ returns a posterior probability of producing the generic, for each level of prevalence. 
We sample a response (*agree* vs. *disagree*) from this posterior distribution for each prevalence level, simulating a single subject's data.
As with the human data, we took the trials where the model agreed with the generic, and took the mean of the prevalence levels corresponding to those trials, giving us the average prevalence at which the model assented to the generic.
We repeated this for each type of property 40 times to simulate a sample of 40 participants. 
We repeated this procedure 1000 times to bootstrap 95% confidence intervals.

The generic endorsement model predicted that *average truth conditions* should not vary appreciably across the different types of properties, consistent with the fact that generics are acceptable for broad range of prevalence levels for all property types.
A similar absence of a gradient was observed in the human data ($\beta = 2.82; SE = 4.02; t(39) = 0.70; p = 0.49$; \red{Figure X}). 
Interpretations of generic utterances are stronger than their average truth conditions for the biological properties but not for the accidental properties (Figure \ref{fig:exp2b}) with both human data, replicating @Cimpian2010, and the model; the extent of the difference is governed by prior property knowledge (mean prevalence when present $\gamma$, from Expt.~2a).
The listener and speaker pair of models predicts human endorsements and interpretations of novel generic utterances well ($r^2(10) = 0.87$, MSE = 0.008).
Thus, our model predicts that the asymmetry between truth conditions and implied prevalence should hold, but only for properties with the most extreme prior beliefs.

In this study [as in @Cimpian2010's], *average truth conditions* is computed on a participant-wise basis but at the level of property type, not property. 

### Extended analysis: Average truth conditions 

<!-- 
  - [1.] do you get similar results by bootstrapping subjects and not going straight to ATC (property type level)?
  - [2.] if so, can you do bootstrapping at the individual property level?
  - [3.] can you build a bayesian model of these data? 
-->
    
# Experiment 2: Property-specific asymmetries

In this experiment, we measure *average truth conditions* for the larger and more diverse stimulus set used in Ch. 2, Expt. 2.

## Method

### Participants

We recruited X participants from MTurk. 
This number was arrived at with the intention of getting approximately X ratings for each unique item in the experiment.
The experiment took on average X minutes and participants were compensated \$K.

### Materials

We used the stimulus set of 75 properties used in Ch. 2, Expt. 2.
Items were generated by considering six different classes of properties: physical characteristics (e.g., *have brown spots*), psychological characteristics (e.g., *experience emotions*), dietary habits (e.g., *eat human food*), habitat (e.g., *live in zoos*), disease (e.g., *get cancer*, *carry malaria*), reproductive behavior (e.g., *have a menstrual cycle*), and other miscellaneous behaviors (e.g., *pound their chests to display dominance*, *perform in the circus*); online sources about strange animal behaviors were consulted in order to find the more obscure properties. 

In addition, participants saw prevalence levels ranging from 10%, 30%, 50%, 70%, and 90%, as in @Cimpian2010 and Expt. 1 above.
In order to avoid fatigue and/or habituation to the statistics of the task, unlike Expt. 1, the exact prevalence seen on an individual trial was sampled from a (discrete) uniform distribution centered at the prevalence level (10, 30, ..., 90) with a range of $\pm 5\%$.

### Procedure

Participants were told that scientists had recently discovered lots of new animals that we did not know existed.
On each trial, they would learn a fact about a new animal and be asked whether or not they agreed with a sentence.

On each trial, participants read: "Scientists discovered an animal called a *K*. Out of all of the *Ks* on the planet, *X%* of them *F*" (e.g., "Scientists discovered an animal called a jav. Out of all of the javs on the planet, 9% of them have an exquisite sense of smell").
They were then asked if the corresponding bare plural sentence "*Ks F*"  was true or false (e.g., "Javs have an exquisite sense of smell").
Participants responded using radio buttons labeled "True" and "False", the ordering of which was randomized between-subjects.
As in Ch. 2, Expt. 2B, novel animal category names were mostly taken from @Cimpian2010 and similar studies on generic language. 
Each participant completed \red{N} trials, \red{M} at each prevalence level (10, 30, 50, 70, 90).
Each trial asked about a different property.

After the generic interpretation trials, participants a memory check trial wherein they were shown ten properties and asked to click on those they had seen in the experiment.
Following the memory check trials and depending on their ratings in the task, participants completed up to five explanation trials. 
On an explanation trial, participants saw a rating they had given for a property they had either rated as (1) true at prevalence levels 10% or 30% or (2) false at prevalence levels 70% or 90%, and asked if they could explain why they gave the response that they gave. 
(These data were used in an exploratory analysis.)
If participants gave no such ratings, they did not complete any explanation trials. 

### Results

# Discussion


# References

```{r create_r-references}
r_refs(file = "generics.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
