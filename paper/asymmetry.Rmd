---
title             : "The asymmetry between generic truth conditions and implied prevalence"
shorttitle        : "Generic asymmetry"

author: 
  - name          : "Michael Henry Tessler"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Serra Mall, Bldg. 420, Rm. 316, Stanford, CA 94305"
    email         : "mhtessler@stanford.edu"
  - name          : "Noah D. Goodman"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"
    
header-includes:
  - \usepackage{tabularx}
  - \usepackage{multicol}
  - \usepackage{wrapfig}
  - \usepackage{caption}
  - \usepackage{booktabs}
  - \usepackage{amsmath}
  - \usepackage{graphicx}
  - \usepackage{subcaption}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  
author_note: >
  This manuscript is currently in prep. Comments or suggestions should be directed to MH Tessler.

abstract: |
  Enter abstract here. Each new line herein must be indented, like this line.
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["generics.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no

class             : "man"
output            : papaja::apa6_pdf
---
\newcommand{\denote}[1]{\mbox{ $[\![ #1 ]\!]$}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\definecolor{Red}{RGB}{255,0,0}
\definecolor{Green}{RGB}{10,200,100}
\definecolor{Blue}{RGB}{10,100,200}

\newcommand{\mht}[1]{{\textcolor{Blue}{[mht: #1]}}}
\newcommand{\ndg}[1]{{\textcolor{Green}{[ndg: #1]}}}
\newcommand{\red}[1]{{\textcolor{Red}{#1}}}


```{r load_packages, include = FALSE}
library("papaja")
library(tidyverse)
library(cowplot)
library(ggthemes)
library(RColorBrewer)
library(ggpirate)
theme_set(theme_few())
```

```{r analysis_preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, cache=T, message=FALSE, sanitize = T)

```

# Introduction

\red{look at things that cite Cimpian et al. (2010) and see if people are discussing the asymmetry in the philosophical literature}

<!-- 
- Note about large-scale truth judgment task: 
  - Cimpian fudges the analysis a little bit: He does it on a subject-wise basis, but using different properties (participants dont rate purple feathers for 10%, 30%, ...)... if we want to do item-wise, we either need to give subjects all the prevalence levels, or do some fancy hierarchical bayesian modeling (the latter may be preferred)
-->


Generic statements (e.g., "Birds fly") convey generalizations about categories [@Carlson1977; @Carlson1995; @Leslie2008].
This kind of language has caught the attention of psychologists, linguists, and philosophers beacuse it is widely prevalent in everyday conversation and child-directed speech [@Gelman1998; @Gelman2008; @GelmanEtAl2004], while at the same time having formal properties that are difficult to characterize [@Carlson1995]. 

Generic statement can be intuitively true under a wide range of statistical conditions. 
"Birds fly" is true of most birds, but "Ducks lay eggs" is only true of adult, fertile, female ducks, which make up less than 50\% of the category.
"Mosquitos carry malaria" is intuitively true despite less than 1\% of category actually having the property.
Despite these variable *truth conditions*, it has been noted that generic statements often carry the *implied prevalence* of a universal or near-universal quantifier [@Abelson1966; @Gelman2002; @Cimpian2010]. 
When both children and adults learn "Bears like to eat ants", they tend to think that *almost all* bears like to eat ants [@Gelman2002].

Thus, there appears to be a surprising d\'{e}colage between the truth conditions and interpretations of generic language: Interpretations are often strong while truth conditions are flexible. 
@Cimpian2010 found that upon reading a generic (e.g., "Glippets have yellow fur."), participants infer that the *implied prevalence* is high (e.g., almost all glippets have yellow fur).
By contrast, participants endorse generics for a wide range of prevalence levels (e.g., even when 30\% of glippets have yellow fur). 
By quantitatively comparing the two measurements, @Cimpian2010 found an asymmetry between between truth conditions and implied prevalence of generic statements but not for quantified statementse involving *all* or *most*.

Not all generics have strong interpretations, however. 
"Mosquitos carry malaria" really seems to only have the quantificational force of an existential claim, meaning *some mosquitos carry malaria*.
Weak interpretations from generics have been observed in tasks having participants draw inferences about individual members of a kind [@Khemlani2009; @Khemlani2012] as well as asking about implied prevalence [@Cimpian2010; Expt. 3].
Coupled with the truth conditions measurements, @Cimpian2010 found that a significant reduction in the asymmetry for generics about accidental properties (e.g., "Glippets have wet fur.").

When and how will the asymmetry between truth conditions and implied prevalence manifest?
<!-- \red{[check cimpian's discussion section]} PERHAPS ADDRESS LATER WHAT CIMPIAN SAYS, because it's not directly the point (his ideas are tooo vague) -->
Here, we draw on the same computational framework to address this question. 
We show through simulation various conditions under which the asymmetry appears and when it is predicted to disappear or even reverse.
Then, we replicate the original asymmetry findings of @Cimpian2010 while expanding the stimulus set to reveal even more variability in the asymmetry.
We then extend these findings to a larger-scale study using a more diverse stimulus set.
In addition to revealing the quantitative intracacies of the asymmetry, our very same model is able to account for both truth conditions and implied prevalence simultaneously. 

# The asymmetry between truth conditions and interpretations

The empirical asymmetry between truth conditions and interpretations was reported by @Cimpian2010.
Truth conditions data result from an endorsement task, a two-alternative forced choice (2AFC) paradigm (as explored in Chapter 3).
Interpretation data are prevalence ratings, typically on a 101-pt scale (e.g., the percentage of the category with the property).
Thus, these two constructs are not in the same measurement space; one must be transformed into the other in order to compare them.

To accomplish this, @Cimpian2010 took the 2AFC endorsement data and computed the average prevalence level at which participants endorse the generic (*average truth conditions*).
<!-- *Average truth conditions* is analagous to an average threshold.  -->
To compute a single participant's average truth conditions, the endorsement data is filtered to look at only the trials where a participant endorsed the generic. 
Using only the endorsed trials, the mean of the referent prevalence levels (empirically supplied to participants) is computed (for each participant separately).
For example, if a participant endorsed the generic at 50%, 70%, and 90% prevalence levels, their *average truth conditions* would be 70%. 
Notice that as a participant becomes more lenient with the generic (endorsing it at lower prevalence levels), their corresponding average truth conditions will lower.
The theoretical minimum average truth conditions is 50%, for a participant who endorsed the generic at all prevalence levels.^[
  Techinically, average truth conditions could be less than 50% were a participant to endorse the statement at low prevalence levels and then reject the statement at high prevalence levels. We do not expect this behavior for generics about positive properties. 
]
If a participant fails to endorse the generic for all prevalence levels, @Cimpian2010 assigned them an average truth conditions of 100%, because presumably the participant would only endorse the generic if it were true of 100% of members.

<!-- 
Technical aspects of the asymmetry:
  - Theoretical exploration using "some", "most" as simply threshold-functions (no speaker) and Uniform priors for the interpretation model
    - This will show the symmetry for both?
    - What will it show for the generic? (what about an L1?)
  - For some, most: interpretation is potentially more complicated (some has the not all [not most?] implicature, which Cimpian finds... "most" maybe as well?)
  - The dynamic range for the asymmetry is more limited for ATC than IP, so must of the action happens because of IP
  
  - Cimpian did it subject-wise but not property-wise (assuming property classes and equivalence within a class...). How do you do it propery-wise?
    - [ ] do you get similar results by bootstrapping?
    - [ ] can you build a bayesian model of these data?
    - one possibility is to have subjects rate all prevalence levels... but this may be strange for people
    - another is to share statistical strength
      - assume there is a threshold for each property
      - each participant's threshold is a sample from this distribution...
      - but properties are completely independent? still may be too sparse
        - maybe you don't need the participant level: just have a threshold model that has the possibility of noise?
-->

\red{Figure 1} shows the average truth conditions for a variety of semantic hypotheses.

```{r asymmetrySimulations, fig.width=8, fig.asp=0.5, cache=F, fig.cap="Model simulations"}

load(file = "cached_results/modelSims-priors_fixedT_uncertainT.RData")
get.colors <- function(pal) brewer.pal(brewer.pal.info[pal, "maxcolors"], pal)
spectrum.color.palette <- get.colors("Blues")
load("cached_results/modelSims-asymmetry.RData")

fig.sims.priors <- ggplot(sims.combined %>%
         filter(
           PriorShape %in% c("uniform", "biological_rare", "accidental_rare"),
           src %in% c("priors")
           ) %>%
         mutate(PriorShape = factor(PriorShape, levels = c("uniform", "biological_rare", "accidental_rare"),
                                    labels = c("uniform", "biological", "accidental")),
                src = factor(src, levels = c( "priors"),
                             labels = c(
                               '\n prevalence prior'
                                        ))), 
       aes(x = state))+
    geom_density(aes(y = ..scaled..), fill= 'black', size = 0.6, alpha = 0.7, adjust = 1.1)+
    theme_few() +
    scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    ylab("Probability density (scaled)") +
    xlab("Prior Prevalence")+
    #scale_color_solarized()+
    #scale_fill_solarized()+
    facet_grid(PriorShape~src, scales = 'free')+
    theme(strip.text.y = element_text(angle = 0),
          legend.position = "none"
          )


fig.sims.distributions <- ggplot(sims.combined %>%
         filter(
           PriorShape %in% c("uniform", "biological_rare", "accidental_rare"),
           src == "posteriors"
           #src %in% c("fixed_0.1", "fixed_0.33", "fixed_0.5", "posteriors")
           ) %>%
         mutate(PriorShape = factor(PriorShape, levels = c("uniform", "biological_rare", "accidental_rare"),
                                    labels = c("Xs Y\n[uniform]", "Xs fly\n[biological]", "Xs carry malaria \n [accidental]")),
                src = factor(src, levels = c("posteriors"),
                             labels = c('generic\n(uncertain threshold)'))),
                # src = factor(src, levels = c( "fixed_0.1", "fixed_0.33",
                #                               "fixed_0.5", "posteriors"),
                #              labels = c(
                #            #    'prevalence prior',
                #                '"some"\n(threshold = 0.01)',
                #                '"more than a third"\n(threshold = 0.33)',
                #               '"most"\n(threshold = 0.5)',
                #                'generic\n(uncertain threshold)'
                #                         ))), 
       aes(x = state, fill = src, color = src))+
    geom_density(aes(y = ..scaled..), 
                 color ='black', fill = 'grey50', size = 0.6, alpha = 0.7, adjust = 1.1)+
    theme_few() +
    scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    ylab("Scaled posterior density") +
    xlab("Implied Prevalence")+
    # scale_fill_brewer(palette=2, type = "seq", direction = -1)+
    # scale_color_brewer(palette=2, type = "seq", direction = -1)+
    #scale_color_manual( values =  c(spectrum.color.palette[c(8,5,2)], "#238b45"))+
    #scale_fill_manual( values = c(spectrum.color.palette[c(8,5,2)], "#238b45") )+
    facet_grid(PriorShape~src, scales = 'free')+
    theme(strip.text.y = element_blank(),# element_text(angle = 0, size = 12),
          legend.position = "none",
          axis.title.y = element_blank())

fig.sims.truthJudgments <- s1.simulations %>%
  filter(model == "generic") %>%
  mutate(prior = factor(prior, levels = c("uniform", "biological_rare", "accidental_rare"),
                                    labels = c("uniform", "biological", "accidental"))) %>%
  ggplot(., aes ( x = referent_prevalence, y = endorsement,
                  linetype = prior))+
  geom_point()+
  geom_line()+
  theme(legend.position = c(0.8, 0.4) ,
        legend.title = element_blank())+
    scale_x_continuous(breaks = c(0, 0.5, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 0.5, 1), limits= c(0, 1))+
  ylab("Endorsement probability")+
  xlab("Referent prevalence")


fig.sims.asymmetry <- m.gen.asym %>%
  filter(src == "posteriors") %>%
  mutate(src = factor(src),
         PriorShape = factor(PriorShape, levels = c("biological_rare","accidental_rare",   "uniform"),
                                    labels = c( "biological",  "accidental","uniform")),
         model = factor(model, levels = c("S1", "L0"), labels = c("Truth conditions", "Implied prevalence"))) %>%
  ggplot(., aes(x = PriorShape, y = expval, 
                fill = model))+
  scale_fill_solarized() + 
      scale_y_continuous(breaks = c(0, 0.5, 1), limits= c(0, 1))+
  ylab("Average prevalence")+
  geom_col(position= position_dodge(), color = 'black')+
  #geom_errorbar(position = position_dodge())+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
        legend.position = c(0.80, 0.85),
        legend.title = element_blank(),
        axis.title.x = element_blank())


cowplot::plot_grid(
  fig.sims.priors + theme(plot.margin = unit(c(6, 3, 6, 0), "pt")), 
  fig.sims.distributions + theme(plot.margin = unit(c(6, 6, 6, 0), "pt")),
  cowplot::plot_grid(
      fig.sims.truthJudgments + theme(plot.margin = unit(c(18, 3, 6, 0), "pt")), 
      fig.sims.asymmetry + theme(plot.margin = unit(c(6, 0, 6, 0), "pt")),
      ncol = 1,
      labels = c("C","D"),
      rel_heights  = c(1, 1.25)
      ),
  #fig.sims.bars+ theme(plot.margin = unit(c(6, 0, 0, 0), "pt")),
  nrow = 1,
  labels = c("A", "B", ""),
  rel_widths = c(0.8, 0.6, 1)
)
```
<!-- - Model figure: -->
<!--   - Prior1, L("Generic1"), S: x=%, y= endorse, group = prior,  ATC & IP: bar plot (x = model) -->
<!--   - Prior2, L("Generic2"), S: x=%, y= endorse,  ATC & IP: bar plot (x = model) -->
<!--   - Prior3, L("Generic3"), S: x=%, y= endorse,  ATC & IP: bar plot (x = model) -->

<!-- 
Computing ATC for model should just be the mean of the production probabilities...
This probably is the same as the Cimpian sampling method, though won't have a way of constructing the variance..
thus the sampling method may be preferred for constructing the variance
-->

The computational model of generic interpretation predicts that the implied prevalence of a generic statement is an interaction between background knowledge (formalized as a prevalence prior) and an underspecified threshold semantics. 
I have shown in Chapter 2 how this model predicts context-sensitive interpretations of generic sentences. 
In Chapter 3, we saw how a speaker model can account for endorsement beahvior.
The implied prevalence of a generic may be very strong (e.g., "Wugs have four legs") or very weak (e.g., "Lorches live in zoos").
The implied prevalence of a generic 

# Experiment 1: Replication and extension of Cimpian et al. (2010) truth conditions task

In Chapter 2, I replicated @Cimpian2010's implied prevalence paradigm using a slightly expanded stimulus set (Expt. 1B). 
To replicate the asymmetry findings, we must also measure endorsements using @Cimpian2010's *truth conditions* paradigm.
Here, we replicate the truth conditions task using the slightly expanded stimulus set of @Cimpian2010.
The relevant prevalence priors have already been measured in Chapter 2 (Expt. 1A).

## Method

### Participants

We recruited 40 participants over MTurk.  
We chose a sample size at least twice as large as the original study by @Cimpian2010 (original $n = 20$), and to match that of Ch. 2 Expt. 1B. 
All participants were self-reported native English speakers and none completed the analagous implied prevalence task (Ch. 2 Expt. 1B).
The experiment took about 5 minutes and participants were compensated \$0.60.

### Procedure and materials

The cover story and materials were the same as in Ch. 2, Expt. 1B and the procedure followed that of @Cimpian2010 Expt. 1.
On each trial, participants were given a statement about a property's prevalence within a novel kind (*referent prevalence*; e.g. "50% of feps have yellow fur."). 
Participants were then asked whether or not they agreed or disagreed with the corresponding generic sentence (e.g. "Feps have yellow fur."). 
Referent prevalence varied between 10, 30, 50, 70, and 90%.

The experiment consisted of 25 trials: 5 trials for each of 5 types of properties measured in Ch. 2, Expt. 1A (part, color part, vague part, common accidental, rare accidental). 
Each prevalence level appeared once for each property type (5 prevalence levels x 5 property types). 

### Results

```{r figCbgResults, fig.width=10.5, fig.asp = 0.35, fig.cap="Words"}
load(file = "cached_results/cbg_results.RData")

dodge_width <- 7

fig.cbg.tj <- df.c.endorse_by_prev %>%
  mutate("Property type" = factor(stim_type,
                                         levels = c("part",
                                                    "vague",
                                                    "color",
                                                    "accidental"),
                                         labels = c("body part",
                                                    "color adj + part",
                                                    "gradable adj + part",
                                                    "accidental"))) %>%
  ggplot(., aes( x = stim_prevalence, y = mean, ymin = ci_lower,
                 ymax = ci_upper, fill = `Property type`,
                 color = `Property type`))+
  geom_line(position = position_dodge(dodge_width), alpha = 0.4,
            linetype = 3)+
  geom_linerange(position = position_dodge(dodge_width), alpha = 0.6,
                 size = 1)+
  geom_point(position = position_dodge(dodge_width),
             size = 2.5, shape = 21, color = 'black')+
  scale_fill_solarized()+
  scale_color_solarized()+
  ylab("Proportion endorse")+
  xlab("\n Referent prevalence")+
  scale_x_continuous(limits = c(0, 100), breaks = c(10,  30, 50, 70,90))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  theme(legend.position = c(0.75, 0.27))+
  coord_fixed(ratio = 65)

dodge_width <- 0.8

fig.cbg.asym <- bind_rows(
  df.c.int.bs %>% mutate(src = 'implied prevalence'),
  df.atc %>% mutate(src = 'truth conditions')
) %>%
  mutate("Property type" = factor(stim_type,
                                         levels = c("part",
                                                    "vague",
                                                    "color",
                                                    "accidental"),
                                         labels = c("body part",
                                                    "color adj + part",
                                                    "gradable adj + part",
                                                    "accidental")),
         src = factor(src, levels = c("truth conditions",
                                      "implied prevalence"))) %>%
  ggplot(., aes( x = `Property type`, y = mean, ymin = ci_lower,
                 ymax = ci_upper, fill = `Property type`, alpha = src))+
  geom_col(position = position_dodge(dodge_width), width = dodge_width,
           color = 'black')+
  geom_errorbar(position = position_dodge(dodge_width),
                 size = 1, width = 0.1)+
  scale_fill_solarized()+
  scale_alpha_manual(values = c(0.6, 1))+
  scale_y_continuous(limits = c(0, 100))+
  ylab("Average truth conditions")+
  guides(fill = F)+
  theme(axis.text.x = element_text(angle = 45, vjust =1 ,hjust = 1),
        legend.title = element_blank(),
        axis.title.x = element_blank()#,
        #legend.position = c(0.75, 0.9)
        )

cowplot::plot_grid(
  fig.cbg.tj + theme(plot.margin = unit(c(6, 3, 30, 0), "pt")), 
  fig.cbg.asym + theme(plot.margin = unit(c(6, 6, 6, 0), "pt")),
  #fig.sims.bars+ theme(plot.margin = unit(c(6, 0, 0, 0), "pt")),
  nrow = 1,
  labels = c("A", "B"),
  rel_widths = c(1,1)
)

```


For both behavioral data and model predictions (Eq. \ref{eq:S1}) we computed the average prevalence that led to an assenting judgment (the *average truth conditions*), for each property type and participant, following the procedure used by @Cimpian2010 and described in the previous section.

For our pair of models, there are two parameters (the two speaker optimality parameters).
We infer them using the same Bayesian data analytic approach as before. 
The MAP and 95\% HPD intervals for $\lambda_1$ is 19.5 [10.5, 19.9] and $\lambda_2$ is 0.4 [0.34, 0.49].
We then subjected the generic endorsement model to the same procedure as the human data. % subjected our model to the same procedure. 

The speaker model $S_2$ returns a posterior probability of producing the generic, for each level of prevalence. 
We sample a response (*agree* vs. *disagree*) from this posterior distribution for each prevalence level, simulating a single subject's data.
As with the human data, we took the trials where the model agreed with the generic, and took the mean of the prevalence levels corresponding to those trials, giving us the average prevalence at which the model assented to the generic.
We repeated this for each type of property 40 times to simulate a sample of 40 participants. 
We repeated this procedure 1000 times to bootstrap 95% confidence intervals.

The generic endorsement model predicted that *average truth conditions* should not vary appreciably across the different types of properties, consistent with the fact that generics are acceptable for broad range of prevalence levels for all property types.
A similar absence of a gradient was observed in the human data ($\beta = 2.82; SE = 4.02; t(39) = 0.70; p = 0.49$; \red{Figure X}). 
Interpretations of generic utterances are stronger than their average truth conditions for the biological properties but not for the accidental properties (Figure \ref{fig:exp2b}) with both human data, replicating @Cimpian2010, and the model; the extent of the difference is governed by prior property knowledge (mean prevalence when present $\gamma$, from Expt.~2a).
The listener and speaker pair of models predicts human endorsements and interpretations of novel generic utterances well ($r^2(10) = 0.87$, MSE = 0.008).
Thus, our model predicts that the asymmetry between truth conditions and implied prevalence should hold, but only for properties with the most extreme prior beliefs.

In this study [as in @Cimpian2010's], *average truth conditions* is computed on a participant-wise basis but at the level of property type, not property. 

### Extended analysis: Average truth conditions 

<!-- 
  - [1.] do you get similar results by bootstrapping subjects and not going straight to ATC (property type level)?
  - [2.] if so, can you do bootstrapping at the individual property level?
  - [3.] can you build a bayesian model of these data? 
-->
    
# Experiment 2: Property-specific asymmetries

In this experiment, we measure *average truth conditions* for the larger and more diverse stimulus set used in Ch. 2, Expt. 2.

## Method

### Participants

We recruited X participants from MTurk. 
This number was arrived at with the intention of getting approximately X ratings for each unique item in the experiment.
The experiment took on average X minutes and participants were compensated \$K.

### Materials

We used the stimulus set of 75 properties used in Ch. 2, Expt. 2.
Items were generated by considering six different classes of properties: physical characteristics (e.g., *have brown spots*), psychological characteristics (e.g., *experience emotions*), dietary habits (e.g., *eat human food*), habitat (e.g., *live in zoos*), disease (e.g., *get cancer*, *carry malaria*), reproductive behavior (e.g., *have a menstrual cycle*), and other miscellaneous behaviors (e.g., *pound their chests to display dominance*, *perform in the circus*); online sources about strange animal behaviors were consulted in order to find the more obscure properties. 

In addition, participants saw prevalence levels ranging from 10%, 30%, 50%, 70%, and 90%, as in @Cimpian2010 and Expt. 1 above.
In order to avoid fatigue and/or habituation to the statistics of the task, unlike Expt. 1, the exact prevalence seen on an individual trial was sampled from a (discrete) uniform distribution centered at the prevalence level (10, 30, ..., 90) with a range of $\pm 5\%$.

### Procedure

Participants were told that scientists had recently discovered lots of new animals that we did not know existed.
On each trial, they would learn a fact about a new animal and be asked whether or not they agreed with a sentence.

On each trial, participants read: "Scientists discovered an animal called a *K*. Out of all of the *Ks* on the planet, *X%* of them *F*" (e.g., "Scientists discovered an animal called a jav. Out of all of the javs on the planet, 9% of them have an exquisite sense of smell").
They were then asked if the corresponding bare plural sentence "*Ks F*"  was true or false (e.g., "Javs have an exquisite sense of smell").
Participants responded using radio buttons labeled "True" and "False", the ordering of which was randomized between-subjects.
As in Ch. 2, Expt. 2B, novel animal category names were mostly taken from @Cimpian2010 and similar studies on generic language. 
Each participant completed \red{N} trials, \red{M} at each prevalence level (10, 30, 50, 70, 90).
Each trial asked about a different property.

After the generic interpretation trials, participants a memory check trial wherein they were shown ten properties and asked to click on those they had seen in the experiment.
Following the memory check trials and depending on their ratings in the task, participants completed up to five explanation trials. 
On an explanation trial, participants saw a rating they had given for a property they had either rated as (1) true at prevalence levels 10% or 30% or (2) false at prevalence levels 70% or 90%, and asked if they could explain why they gave the response that they gave. 
(These data were used in an exploratory analysis.)
If participants gave no such ratings, they did not complete any explanation trials. 

### Results

# Discussion


# References

```{r create_r-references}
r_refs(file = "generics.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
